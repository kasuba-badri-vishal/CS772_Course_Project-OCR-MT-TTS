KBSF computational cost becomes a function of m only. In particular, the cost of solving M through dynamic programming becomes polynomial in m instead of n: while one application off, the Bellman operator of M, is O, the computation of T is O. Therefore, KBSF's time and memory complexities are only linear in n. We note that, in practice, KBSF's computational requirements can be reduced even further if one enforces the kernels K7 and KT to be sparse. In particular, given a fixed Sis instead of computing K for j = 1,2,..,nas one can evaluate the kernel on a pre-specified neighborhood of Si only. Assuming that K is zero for all $9 outside this region, one avoids not only computing the kernel but also storing the resulting values for a fixed $9. 4.1 A closer look at KBSF's approximation As outlined in Section 3 KBRL defines the probability of a transition from state s to state 3% as being K9, where a, b E A . Note that the kernel K9 is computed with the initial state SK and not SK itself. The intuition behind this is simple: since we know the transition SR 9 SK has occurred before, the more "similar" 35 is to SR3 the more likely the transition 3 4 SK becomes . From , it is clear that the computation of matrices Ka performed by KBSF follows the same reasoning underlying the computation of KBRL's matrices Pa, in particular, KP gives the probability of a transition from Si to 3RHowever, when we look at matrix D things are slightly different: here, the probability of a "transition" from 35 to representative state Sj is given by R-a computation that involves Sj itself. If we were to strictly adhere to KBRL's logic when computing the transition probabilities to the representative states Sj the probability of transitioning from s5 to Sj upon executing action a should be a function of 33 and a state s' from which we knew a transition s' 9 Sj had occurred. In this case we would end up with one matrix Da for each action a € A. Note though that this formulation of the method. is not practical, because the computation of the matrices Da would require a transition 9 5; for each a € A and each Si € S. Clearly, such a requirement is hard to fulfill even if we have a generative model available In this section we provide an interpretation of the approximation computed by KBSF that supports our definition of matrix D. We start by looking at how KBRL constructs the matrices Pa As shown in Figure 2a, for each action a € A the state 89 has an associated stochastic vector D9 E Rlxn whose nonzero entries correspond to the kernel K7 evaluated at SRk= 1,2,.. : MaSince we are dealing with a continuous state space, it is possible to compute an analogous vector for any S € S and any a € A. Focusing on the to generate sample transitions. nonzero entries of D, we define the function Clearly, full knowledge of the function Psa allows for an exact computation of KBRL's transition matrix Pa Now suppose we do not know Psa and we want to compute an 13