tions between dimensions does not compensate for the increased expressiveness of the model due to the larger hidden state size. Further experimentation, with tuned hyperparameters, is needed to determine the actual benefits of predefined sparseness, in terms of model size, resulting perplexity, and sensitivity to the choice of hyperparameters. 4 Sparse Word Embeddings Given a vocabulary with V words, we want to construct vector representations of length k for each word such that the total number of parameters needed , is smaller than et et Table 1: Language modeling for = 1725. Dense and sparse variants have kV. We introduce one way to do this based on the same number of parameters for N = 3 and word frequencies , and present part-of7 0.555. These values are obtained by identispeech tagging experiments . fying both expressions. Note that the equality in 4.1 Word-Frequency based Embedding Size holds only approximately due to rounding errors amounts to deciding which positions in the word Figure 1 displays Whh and Whi for the midzero, prior to training. We define the fraction of dle layer, which has close to 11M parameters out trainable entries in E: as the embedding density OE. of the total of 24M in the whole model. A dense We hypothesize that rare words can be represented model with hidden size h = 1725 would require with fewer parameters than frequent words, since 46M parameters, with 24M in the middle LSTM they only appear in very specific contexts. This Given the strong hyperparameter dependence Word occurrence frequencies have a typical Zipof the AWD-LSTM model, and the known isfian nature , with many rare sues in objectively evaluating language models and few highly frequent terms. Thus, representing , we decided to keep all hythe long tail of rare terms with short embeddings perparameters 5, including the weight dropping OE, we want to save on the rare words, in terms of with P = 0.5 in the sparse Whh matrices. Taassigning trainable parameters, and focus on the ble 1 shows the test perplexity on a processed fewer more popular words. An exponential decay version of the Penn Treein the number of words that are assigned longer bank , both with and representations is one possible way to implement without the finetune' steps, displaying mean and this. In other words, we propose to have the numstandard deviation over 5 different runs. Withber of words that receive a trainable parameter at out finetuning, the sparse model consistently perdimension j decrease with a factor Q3 . forms around 1 perplexity point better, whereas afFor a given fraction OE, the parameter a can be deter finetuning, the original remains slightly better, termined from requiring the total number of nonalthough less consistently SO over different ranzero embedding parameters to amount to a given model parameters for the dense and sparse case Predefined sparseness in word embeddings embedding matrix E E RVxk should be fixed to in and . alone. will be investigated experimentally in Section tion scheme as in the implementation from MerIn the case of a low desired embedding density dom seeds. We observed that the sparse model fraction OE of all parameters: overfits more strongly than the baseline, especially during the finetune step. We hypothesize that the regularization effect of a priori limiting interacSOur implementation extends https: github. com/salesforce/awd-lstm-1m. 6The 'finetune' step indicates hot-starting the Averaged Gradient Descent optimization once. more, after convergence in the initial optimization step . matrices Figure gives examples of embedding with varying OE. For a vocabulary of 44k terms