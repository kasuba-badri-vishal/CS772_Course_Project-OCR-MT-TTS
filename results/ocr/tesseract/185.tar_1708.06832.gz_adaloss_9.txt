block para line word confidence X1 Y1 X2 Y2 token
1 1 1 1 96 301 211 335 227 not
1 1 1 2 96 346 208 444 227 saturated
1 1 1 3 91 454 208 480 233 by
1 1 1 4 91 491 209 591 227 ResANN
1 1 1 5 95 602 208 635 230 26,
1 1 1 6 96 647 208 679 227 the
1 1 1 7 96 690 208 748 233 delay
1 1 1 8 96 760 214 842 233 appears
1 1 1 9 96 854 208 973 233 significant.
1 1 1 10 93 991 208 1067 230 Hence,
1 1 1 11 92 1079 209 1165 227 EANNs
1 1 1 12 96 1176 214 1222 233 may
1 1 1 13 96 1233 211 1267 227 not
1 1 1 14 96 1277 208 1302 227 be
1 1 1 15 96 1313 208 1345 227 the
1 1 1 16 96 1355 208 1400 227 best
1 1 2 1 96 301 238 359 257 when
1 1 2 2 96 368 238 400 257 the
1 1 2 3 96 408 238 546 263 performance
1 1 2 4 96 555 238 572 257 is
1 1 2 5 96 580 242 615 257 not
1 1 2 6 97 623 238 721 257 saturated
1 1 2 7 97 729 244 752 257 or
1 1 2 8 96 760 238 818 257 when
1 1 2 9 96 827 238 858 257 the
1 1 2 10 96 867 241 958 257 constant
1 1 2 11 97 967 238 1051 257 fraction
1 1 2 12 96 1059 238 1083 257 of
1 1 2 13 96 1089 242 1143 257 extra
1 1 2 14 95 1151 242 1194 257 cost
1 1 2 15 95 1203 238 1219 257 is
1 1 2 16 96 1228 238 1309 257 critical.
2 1 1 1 63 300 284 334 303 5.4
2 1 1 2 63 363 284 460 303 Data-set
2 1 1 3 96 468 284 577 309 Difficulty
2 1 1 4 96 586 290 659 304 versus
2 1 1 5 96 667 284 773 309 Adaptive
2 1 1 6 96 782 284 875 309 Weights
3 1 1 1 95 300 340 322 358 In
3 1 1 2 95 333 332 374 373 Fig.
3 1 1 3 70 383 332 422 373 [5c]
3 1 1 4 96 430 345 460 358 we
3 1 1 5 96 471 339 513 364 plot
3 1 1 6 96 523 339 555 358 the
3 1 1 7 93 567 339 614 358 final
3 1 1 8 92 625 339 722 358 AdaLoss
3 1 1 9 96 733 339 817 364 weights
3 1 1 10 97 828 339 852 358 of
3 1 1 11 96 861 339 893 358 the
3 1 1 12 93 905 345 959 358 same
3 1 1 13 92 970 339 1069 358 ResANN
3 1 1 14 96 1081 339 1149 358 model
3 1 1 15 96 1160 339 1238 363 (25,32)
3 1 1 16 91 1249 345 1276 358 on
3 1 1 17 58 1286 339 1398 362 CIFAR10,
3 1 2 1 88 300 370 426 393 CIFAR100,
3 1 2 2 92 438 370 477 389 and
3 1 2 3 92 488 370 567 393 SVHN,
3 1 2 4 97 579 370 599 389 in
3 1 2 5 97 609 370 667 389 order
3 1 2 6 96 677 373 697 389 to
3 1 2 7 96 708 370 765 395 study
3 1 2 8 96 776 370 808 389 the
3 1 2 9 96 819 370 890 389 effects
3 1 2 10 96 901 370 925 389 of
3 1 2 11 93 933 370 965 389 the
3 1 2 12 92 976 370 1071 389 data-sets
3 1 2 13 96 1082 376 1108 389 on
3 1 2 14 96 1119 370 1151 389 the
3 1 2 15 95 1162 370 1252 395 weights.
3 1 2 16 96 1270 371 1304 389 We
3 1 2 17 96 1315 370 1399 389 observe
3 1 3 1 96 301 400 341 419 that
3 1 3 2 96 349 400 402 419 from
3 1 3 3 96 410 400 442 419 the
3 1 3 4 93 451 400 524 419 easiest
3 1 3 5 92 532 400 623 423 data-set,
3 1 3 6 92 633 400 712 423 SVHN,
3 1 3 7 96 722 403 742 419 to
3 1 3 8 96 750 400 782 419 the
3 1 3 9 93 791 400 876 423 hardest,
3 1 3 10 91 885 400 1011 423 CIFAR100,
3 1 3 11 96 1020 400 1052 419 the
3 1 3 12 96 1062 400 1145 425 weights
3 1 3 13 96 1154 406 1186 419 are
3 1 3 14 96 1195 406 1250 419 more
3 1 3 15 96 1259 400 1399 419 concentrated
3 1 4 1 96 300 436 327 449 on
3 1 4 2 96 336 430 368 449 the
3 1 4 3 96 378 430 425 449 final
3 1 4 4 96 435 430 505 455 layers.
3 1 4 5 97 518 430 566 449 This
3 1 4 6 96 576 433 667 455 suggests
3 1 4 7 93 677 430 717 449 that
3 1 4 8 92 726 430 822 449 AdaLoss
3 1 4 9 96 832 436 869 449 can
3 1 4 10 96 878 430 1027 455 automatically
3 1 4 11 96 1036 430 1107 449 decide
3 1 4 12 96 1116 430 1157 449 that
3 1 4 13 92 1165 430 1235 449 harder
3 1 4 14 92 1244 430 1339 449 data-sets
3 1 4 15 96 1348 430 1399 449 need
3 1 5 1 96 301 467 356 480 more
3 1 5 2 95 364 461 505 480 concentrated
3 1 5 3 96 514 461 561 480 final
3 1 5 4 96 570 461 654 486 weights
3 1 5 5 96 663 465 683 480 to
3 1 5 6 96 691 461 741 480 have
3 1 5 7 96 750 461 889 486 near-optimal
3 1 5 8 96 898 461 945 480 final
3 1 5 9 96 954 461 1099 486 performance,
3 1 5 10 96 1108 461 1197 480 whereas
3 1 5 11 96 1206 467 1232 480 on
3 1 5 12 93 1240 467 1288 486 easy
3 1 5 13 92 1297 461 1398 484 data-sets,
3 1 6 1 96 301 497 356 510 more
3 1 6 2 96 364 491 434 510 efforts
3 1 6 3 96 442 497 474 510 are
3 1 6 4 96 482 491 570 510 directed
3 1 6 5 96 578 494 598 510 to
3 1 6 6 96 605 491 659 516 early
3 1 6 7 96 667 491 794 516 predictions.
3 1 6 8 92 806 492 881 513 Hence,
3 1 6 9 92 889 491 986 510 AdaLoss
3 1 6 10 96 994 491 1078 516 weights
3 1 6 11 96 1087 497 1133 516 may
3 1 6 12 96 1140 491 1223 516 provide
3 1 6 13 96 1231 491 1360 510 information
3 1 6 14 96 1368 491 1400 510 for
3 1 7 1 96 300 521 437 546 practitioners
3 1 7 2 96 446 525 466 540 to
3 1 7 3 96 474 521 545 546 design
3 1 7 4 96 554 521 592 540 and
3 1 7 5 96 600 521 676 540 choose
3 1 7 6 96 684 521 762 540 models
3 1 7 7 96 771 521 833 540 based
3 1 7 8 93 841 527 867 540 on
3 1 7 9 91 875 521 977 540 data-sets.
4 1 1 1 93 301 566 316 589 6
4 1 1 2 93 351 566 508 590 Conclusion
4 1 1 3 95 518 566 569 590 and
4 1 1 4 95 579 566 727 590 Discussion
5 1 1 1 96 300 631 348 650 This
5 1 1 2 96 360 631 415 650 work
5 1 1 3 96 426 631 505 650 devises
5 1 1 4 96 517 631 589 656 simple
5 1 1 5 96 601 631 691 656 adaptive
5 1 1 6 93 703 631 794 656 weights,
5 1 1 7 92 806 631 909 654 AdaLoss,
5 1 1 8 96 922 631 954 650 for
5 1 1 9 96 964 631 1049 656 training
5 1 1 10 96 1060 631 1147 656 anytime
5 1 1 11 96 1158 631 1280 656 predictions
5 1 1 12 93 1292 631 1312 650 in
5 1 1 13 92 1323 632 1398 650 DNNs.
5 1 2 1 96 301 663 335 681 We
5 1 2 2 96 345 662 428 687 provide
5 1 2 3 95 437 662 528 687 multiple
5 1 2 4 96 538 662 653 681 theoretical
5 1 2 5 96 663 662 790 681 motivations
5 1 2 6 96 801 662 832 681 for
5 1 2 7 96 842 662 890 681 such
5 1 2 8 96 900 662 991 687 weights,
5 1 2 9 96 1002 662 1040 681 and
5 1 2 10 96 1050 662 1105 681 show
5 1 2 11 96 1115 662 1279 687 experimentally
5 1 2 12 93 1290 662 1330 681 that
5 1 2 13 93 1339 662 1399 687 adap-
5 1 3 1 96 301 692 339 711 tive
5 1 3 2 96 349 692 433 717 weights
5 1 3 3 96 443 692 514 711 enable
5 1 3 4 96 524 692 582 711 small
5 1 3 5 96 591 692 660 711 ANNs
5 1 3 6 96 670 696 690 711 to
5 1 3 7 96 699 692 823 717 outperform
5 1 3 8 96 832 692 885 717 large
5 1 3 9 96 894 692 963 711 ANNs
5 1 3 10 96 974 692 1021 711 with
5 1 3 11 96 1031 692 1063 711 the
5 1 3 12 95 1072 692 1189 717 commonly
5 1 3 13 93 1199 692 1248 711 used
5 1 3 14 92 1257 692 1399 717 non-adaptive
5 1 4 1 96 300 725 392 741 constant
5 1 4 2 95 403 722 493 747 weights.
5 1 4 3 96 513 723 584 741 Future
5 1 4 4 96 596 722 661 741 works
5 1 4 5 96 673 728 699 741 on
5 1 4 6 96 711 722 802 747 adaptive
5 1 4 7 95 814 722 897 747 weights
5 1 4 8 96 909 722 1000 741 includes
5 1 4 9 93 1011 722 1126 747 examining
5 1 4 10 92 1138 722 1234 741 AdaLoss
5 1 4 11 95 1246 722 1278 741 for
5 1 4 12 93 1289 722 1400 741 multi-task
5 1 5 1 96 300 752 402 777 problems
5 1 5 2 96 411 752 449 771 and
5 1 5 3 96 458 752 597 777 investigating
5 1 5 4 95 606 752 631 771 its
5 1 5 5 77 640 752 773 771 “first-order”
5 1 5 6 96 782 752 867 771 variants
5 1 5 7 96 876 752 917 771 that
5 1 5 8 96 925 752 1034 771 normalize
5 1 5 9 96 1042 752 1074 771 the
5 1 5 10 95 1083 752 1147 771 losses
5 1 5 11 97 1156 752 1182 777 by
5 1 5 12 96 1191 752 1301 771 individual
5 1 5 13 96 1310 752 1400 777 gradient
5 1 6 1 96 300 789 368 802 norms
5 1 6 2 96 378 786 398 802 to
5 1 6 3 96 409 783 489 802 address
5 1 6 4 96 500 783 601 802 unknown
5 1 6 5 96 610 783 682 802 offsets
5 1 6 6 96 692 783 716 802 of
5 1 6 7 96 724 783 788 802 losses
5 1 6 8 96 798 789 819 802 as
5 1 6 9 96 830 783 876 802 well
5 1 6 10 96 886 789 907 802 as
5 1 6 11 96 918 783 950 802 the
5 1 6 12 96 960 783 1061 802 unknown
5 1 6 13 96 1071 783 1141 802 scales.
5 1 6 14 96 1157 784 1191 802 We
5 1 6 15 96 1202 783 1245 802 also
5 1 6 16 96 1254 786 1301 802 note
5 1 6 17 96 1311 783 1351 802 that
5 1 6 18 96 1361 783 1399 802 this
5 1 7 1 97 301 813 356 832 work
5 1 7 2 96 364 819 401 832 can
5 1 7 3 96 409 813 434 832 be
5 1 7 4 96 442 813 550 832 combined
5 1 7 5 96 559 813 606 832 with
5 1 7 6 96 614 813 733 838 orthogonal
5 1 7 7 97 742 813 807 832 works
5 1 7 8 93 816 813 836 832 in
5 1 7 9 92 844 813 948 838 early-exit
5 1 7 10 96 956 813 1056 838 budgeted
5 1 7 11 96 1064 813 1185 838 predictions
5 1 7 12 96 1197 812 1325 840 (Guan
5 1 7 13 93 1263 808 1284 844 et
5 1 7 14 85 1293 808 1329 844 al.|
5 1 7 15 33 1329 805 1403 847 2017}
6 1 1 1 92 306 845 411 862 Bolukbasi
6 1 1 2 92 420 847 439 862 et
6 1 1 3 63 447 840 550 873 al.|/2017)
6 1 1 4 96 559 843 590 862 for
6 1 1 5 96 599 843 668 868 saving
6 1 1 6 96 677 849 760 868 average
6 1 1 7 95 769 846 806 862 test
6 1 1 8 95 814 843 957 868 computation.
7 1 1 1 96 300 893 574 923 Acknowledgements
8 1 1 1 95 1332 808 1400 844 " "
9 1 1 1 96 300 964 348 983 This
9 1 1 2 96 360 964 415 983 work
9 1 1 3 96 427 970 467 983 was
9 1 1 4 96 479 964 592 983 conducted
9 1 1 5 97 603 964 623 983 in
9 1 1 6 97 634 967 676 989 part
9 1 1 7 96 687 964 772 989 through
9 1 1 8 96 783 964 925 983 collaborative
9 1 1 9 96 936 964 1075 989 participation
9 1 1 10 96 1086 964 1106 983 in
9 1 1 11 96 1118 964 1150 983 the
9 1 1 12 96 1161 964 1258 983 Robotics
9 1 1 13 96 1270 964 1399 984 Consortium
9 1 2 1 96 301 995 412 1020 sponsored
9 1 2 2 96 419 995 445 1020 by
9 1 2 3 96 453 995 486 1014 the
9 1 2 4 96 494 995 533 1014 U.S
9 1 2 5 96 542 995 605 1020 Army
9 1 2 6 96 613 995 713 1014 Research
9 1 2 7 96 721 995 842 1020 Laboratory
9 1 2 8 96 851 995 913 1014 under
9 1 2 9 96 920 995 952 1014 the
9 1 2 10 96 960 995 1108 1014 Collaborative
9 1 2 11 96 1116 995 1245 1020 Technology
9 1 2 12 93 1253 995 1345 1014 Alliance
9 1 2 13 92 1353 996 1399 1014 Pro-
9 1 3 1 96 300 1031 362 1050 gram,
9 1 3 2 96 374 1025 507 1050 Cooperative
9 1 3 3 93 517 1025 640 1050 Agreement
9 1 3 4 91 650 1025 881 1044 W911NF-10-2-0016.
9 1 3 5 96 899 1025 941 1044 The
9 1 3 6 96 952 1025 1013 1044 views
9 1 3 7 96 1024 1025 1063 1044 and
9 1 3 8 96 1073 1025 1202 1044 conclusions
9 1 3 9 96 1213 1025 1320 1044 contained
9 1 3 10 96 1330 1025 1350 1044 in
9 1 3 11 96 1361 1025 1399 1044 this
9 1 4 1 96 300 1055 409 1074 document
9 1 4 2 96 417 1061 449 1074 are
9 1 4 3 96 457 1055 514 1074 those
9 1 4 4 97 522 1055 546 1074 of
9 1 4 5 96 552 1055 584 1074 the
9 1 4 6 96 593 1055 672 1074 authors
9 1 4 7 96 681 1055 719 1074 and
9 1 4 8 96 728 1055 800 1074 should
9 1 4 9 96 808 1058 842 1074 not
9 1 4 10 97 850 1055 874 1074 be
9 1 4 11 96 883 1055 1001 1080 interpreted
9 1 4 12 96 1010 1061 1031 1074 as
9 1 4 13 96 1039 1055 1174 1080 representing
9 1 4 14 97 1183 1055 1215 1074 the
9 1 4 15 96 1223 1055 1299 1074 official
9 1 4 16 96 1307 1055 1398 1080 policies,
9 1 5 1 96 300 1085 363 1104 either
9 1 5 2 96 372 1085 480 1110 expressed
9 1 5 3 96 490 1091 512 1104 or
9 1 5 4 96 521 1085 610 1110 implied,
9 1 5 5 97 621 1085 645 1104 of
9 1 5 6 96 653 1085 685 1104 the
9 1 5 7 96 695 1085 759 1110 Army
9 1 5 8 96 769 1085 869 1104 Research
9 1 5 9 96 878 1085 1000 1110 Laboratory
9 1 5 10 96 1010 1085 1034 1104 of
9 1 5 11 96 1042 1085 1074 1104 the
9 1 5 12 96 1085 1085 1131 1104 U.S.
9 1 5 13 96 1142 1085 1284 1104 Government.
9 1 5 14 96 1299 1085 1341 1104 The
9 1 5 15 96 1352 1085 1398 1104 U.S.
9 1 6 1 96 301 1116 437 1135 Government
9 1 6 2 96 448 1116 464 1135 is
9 1 6 3 96 476 1116 591 1135 authorized
9 1 6 4 96 602 1120 622 1135 to
9 1 6 5 96 633 1116 742 1141 reproduce
9 1 6 6 96 753 1116 792 1135 and
9 1 6 7 96 802 1116 905 1135 distribute
9 1 6 8 96 916 1116 999 1141 reprints
9 1 6 9 96 1010 1116 1042 1135 for
9 1 6 10 96 1052 1116 1189 1135 Government
9 1 6 11 91 1199 1122 1295 1141 purposes
9 1 6 12 90 1307 1116 1399 1135 notwith-
9 1 7 1 96 301 1146 393 1171 standing
9 1 7 2 96 401 1152 439 1171 any
9 1 7 3 96 448 1146 553 1171 copyright
9 1 7 4 96 561 1146 650 1165 notation
9 1 7 5 96 658 1146 731 1165 herein.
10 1 1 1 96 301 1196 453 1220 References
11 1 1 1 93 300 1267 332 1287 Ba,
11 1 1 2 92 341 1267 360 1284 L.
11 1 1 3 96 368 1267 382 1284 J.
11 1 1 4 96 391 1267 425 1284 and
11 1 1 5 93 433 1267 520 1287 Caruana,
11 1 1 6 93 528 1267 548 1284 R.
11 1 1 7 97 560 1267 589 1284 Do
11 1 1 8 96 597 1267 642 1289 deep
11 1 1 9 96 650 1270 688 1284 nets
11 1 1 10 96 696 1267 751 1289 really
11 1 1 11 96 759 1267 805 1284 need
11 1 1 12 96 813 1270 830 1284 to
11 1 1 13 96 838 1267 860 1284 be
11 1 1 14 96 868 1267 924 1290 deep?
11 1 1 15 96 936 1267 956 1284 In
11 1 1 16 96 963 1267 1085 1289 Proceedings
11 1 1 17 93 1093 1267 1113 1289 of
11 1 1 18 93 1118 1267 1175 1289 NIPS,
11 1 1 19 96 1184 1267 1237 1284 2014.
12 1 1 1 92 301 1319 377 1341 Bengio,
12 1 1 2 92 384 1319 409 1339 Y.,
12 1 1 3 91 417 1319 525 1339 Louradour,
12 1 1 4 92 533 1319 553 1339 J.,
12 1 1 5 93 561 1319 659 1339 Collobert,
12 1 1 6 92 666 1319 693 1339 R.,
12 1 1 7 96 701 1319 736 1336 and
12 1 1 8 96 743 1319 821 1339 Weston,
12 1 1 9 96 828 1319 842 1336 J.
12 1 1 10 96 851 1319 965 1336 Curriculum
12 1 1 11 96 972 1319 1057 1341 learning.
12 1 1 12 96 1067 1319 1086 1336 In
12 1 1 13 96 1092 1319 1215 1341 Proceedings
12 1 1 14 96 1221 1319 1237 1341 of
12 1 1 15 96 1246 1319 1275 1336 the
12 1 1 16 96 1281 1319 1324 1336 26th
12 1 1 17 96 1331 1319 1400 1336 annual
12 1 2 1 96 329 1347 458 1364 international
12 1 2 2 96 464 1347 572 1369 conference
12 1 2 3 96 579 1353 603 1364 on
12 1 2 4 96 610 1347 693 1364 machine
12 1 2 5 96 701 1347 789 1369 learning,
12 1 2 6 96 798 1347 851 1364 2009.
13 1 1 1 96 301 1399 370 1421 Boddy,
13 1 1 2 96 381 1399 435 1416 Mark
13 1 1 3 96 445 1399 479 1416 and
13 1 1 4 95 489 1399 546 1419 Dean,
13 1 1 5 95 558 1399 642 1416 Thomas.
13 1 1 6 96 663 1399 738 1422 Solving
13 1 1 7 96 749 1399 903 1422 time-dependent
13 1 1 8 96 913 1399 999 1422 planning
13 1 1 9 96 1009 1399 1106 1422 problems.
13 1 1 10 96 1126 1399 1145 1416 In
13 1 1 11 96 1155 1399 1278 1421 Proceedings
13 1 1 12 96 1288 1399 1311 1421 of
13 1 1 13 96 1317 1399 1346 1416 the
13 1 1 14 96 1357 1399 1399 1416 11th
13 1 2 1 96 328 1426 459 1443 International
13 1 2 2 96 465 1427 515 1444 Joint
13 1 2 3 96 522 1426 634 1448 Conference
13 1 2 4 96 641 1432 665 1443 on
13 1 2 5 94 671 1426 760 1448 Artificial
13 1 2 6 93 767 1426 883 1448 Intelligence
13 1 2 7 93 892 1436 897 1438 -
13 1 2 8 96 907 1426 977 1443 Volume
13 1 2 9 91 985 1426 1002 1446 2,
13 1 2 10 0 1010 1426 1108 1446 IJCAV’89,
13 1 2 11 96 1117 1431 1145 1449 pp.
13 1 2 12 86 1154 1426 1245 1446 979-984,
13 1 2 13 95 1256 1426 1307 1443 1989.
14 1 1 1 93 301 1478 406 1498 Bolukbasi,
14 1 1 2 96 416 1478 477 1500 Tolga,
14 1 1 3 96 487 1478 549 1500 Wang,
14 1 1 4 96 559 1478 632 1501 Joseph,
14 1 1 5 96 642 1478 706 1498 Dekel,
14 1 1 6 96 717 1478 765 1498 Ofer,
14 1 1 7 93 776 1478 811 1495 and
14 1 1 8 92 820 1478 925 1500 Saligrama,
14 1 1 9 96 936 1478 1042 1495 Venkatesh.
14 1 1 10 96 1060 1478 1149 1501 Adaptive
14 1 1 11 96 1158 1478 1219 1495 neural
14 1 1 12 96 1229 1478 1318 1495 networks
14 1 1 13 96 1328 1478 1356 1495 for
14 1 1 14 97 1365 1478 1399 1495 fast
14 1 2 1 92 328 1506 414 1523 test-time
14 1 2 2 96 422 1506 526 1529 prediction.
14 1 2 3 92 538 1506 557 1523 In
14 1 2 4 92 564 1506 628 1526 JCML,
14 1 2 5 96 637 1506 690 1523 2017.
15 1 1 1 93 300 1557 339 1577 Cai,
15 1 1 2 91 350 1557 441 1577 Zhaowei,
15 1 1 3 90 452 1557 543 1577 Saberian,
15 1 1 4 93 554 1557 673 1574 Mohammad
15 1 1 5 92 683 1557 703 1577 J.,
15 1 1 6 96 714 1557 749 1574 and
15 1 1 7 96 759 1557 886 1577 Vasconcelos,
15 1 1 8 96 897 1557 956 1574 Nuno.
15 1 1 9 93 974 1557 1063 1579 Learning
15 1 1 10 92 1073 1557 1261 1580 Complexity-Aware
15 1 1 11 96 1270 1557 1361 1574 Cascades
15 1 1 12 97 1371 1557 1400 1574 for
15 1 2 1 96 328 1585 379 1608 Deep
15 1 2 2 96 387 1585 490 1602 Pedestrian
15 1 2 3 96 497 1585 598 1602 Detection.
15 1 2 4 96 609 1585 629 1602 In
15 1 2 5 96 635 1585 767 1602 International
15 1 2 6 96 775 1585 886 1607 Conference
15 1 2 7 96 894 1591 917 1602 on
15 1 2 8 96 926 1585 1024 1607 Computer
15 1 2 9 92 1033 1586 1090 1602 Vision
15 1 2 10 91 1099 1585 1176 1606 (ICCV),
15 1 2 11 96 1185 1585 1238 1602 2015.
16 1 1 1 95 616 1319 620 1654 " "
17 1 1 1 93 301 1637 357 1657 Chen,
17 1 1 2 91 367 1637 452 1657 Minmin,
17 1 1 3 96 462 1637 580 1659 Weinberger,
17 1 1 4 92 590 1637 650 1654 Kilian
17 1 1 5 91 659 1637 687 1658 Q.,
17 1 1 6 96 697 1637 790 1660 Chapelle,
17 1 1 7 93 800 1637 873 1657 Olivier,
17 1 1 8 92 883 1637 958 1657 Kedem,
17 1 1 9 96 968 1637 1010 1657 Dor,
17 1 1 10 96 1020 1637 1054 1654 and
17 1 1 11 93 1063 1637 1097 1657 Xu,
17 1 1 12 91 1107 1637 1201 1659 Zhixiang.
17 1 1 13 96 1216 1637 1310 1654 Classifier
17 1 1 14 96 1318 1637 1399 1654 Cascade
17 1 2 1 96 328 1665 357 1682 for
17 1 2 2 95 363 1665 480 1687 Minimizing
17 1 2 3 96 487 1665 560 1682 Feature
17 1 2 4 96 568 1665 674 1682 Evaluation
17 1 2 5 96 682 1665 731 1682 Cost.
17 1 2 6 92 743 1665 762 1682 In
17 1 2 7 89 768 1665 863 1685 AJSTATS,
17 1 2 8 96 871 1665 925 1682 2012.
18 1 1 1 93 301 1716 357 1736 Chen,
18 1 1 2 91 365 1716 432 1738 Qifeng
18 1 1 3 93 439 1716 473 1733 and
18 1 1 4 93 480 1716 553 1736 Koltun,
18 1 1 5 87 560 1716 642 1733 Vladlen.
18 1 1 6 96 651 1716 783 1739 Photographic
18 1 1 7 96 790 1716 849 1738 image
18 1 1 8 96 856 1716 945 1738 synthesis
18 1 1 9 96 952 1716 995 1733 with
18 1 1 10 96 1001 1716 1090 1733 cascaded
18 1 1 11 96 1096 1716 1202 1733 refinement
18 1 1 12 96 1208 1716 1303 1733 networks.
18 1 1 13 91 1312 1716 1331 1733 In
18 1 1 14 85 1337 1716 1399 1736 JCCV,
18 1 2 1 96 328 1744 382 1761 2017.
19 1 1 1 96 301 1796 368 1816 Grubb,
19 1 1 2 96 378 1796 481 1813 Alexander
19 1 1 3 92 489 1796 524 1813 and
19 1 1 4 90 533 1796 614 1818 Bagnell,
19 1 1 5 96 624 1796 637 1813 J.
19 1 1 6 93 647 1796 729 1813 Andrew.
19 1 1 7 92 745 1796 868 1819 SpeedBoost:
19 1 1 8 96 881 1796 967 1818 Anytime
19 1 1 9 96 975 1796 1076 1813 Prediction
19 1 1 10 96 1085 1796 1128 1813 with
19 1 1 11 93 1137 1796 1222 1813 Uniform
19 1 1 12 91 1230 1796 1398 1819 Near-Optimality.
19 1 2 1 92 328 1824 348 1841 In
19 1 2 2 89 353 1824 449 1844 AISTATS,
19 1 2 3 96 457 1824 510 1841 2012.
20 1 1 1 93 301 1875 359 1895 Guan,
20 1 1 2 89 368 1875 419 1898 Jiaqi,
20 1 1 3 96 429 1875 468 1895 Liu,
20 1 1 4 96 477 1875 533 1897 Yang,
20 1 1 5 96 542 1875 581 1895 Liu,
20 1 1 6 96 591 1875 656 1897 Qiang,
20 1 1 7 97 665 1875 700 1892 and
20 1 1 8 96 708 1875 762 1898 Peng,
20 1 1 9 93 771 1875 816 1892 Jian.
20 1 1 10 92 830 1875 990 1897 Energy-efficient
20 1 1 11 96 998 1875 1096 1892 amortized
20 1 1 12 96 1105 1875 1196 1892 inference
20 1 1 13 97 1205 1875 1248 1892 with
20 1 1 14 96 1256 1875 1345 1892 cascaded
20 1 1 15 96 1353 1875 1399 1898 deep
20 1 2 1 94 328 1903 430 1920 classifiers.
20 1 2 2 93 442 1903 461 1920 In
20 1 2 3 93 468 1904 519 1920 arxiv
20 1 2 4 93 524 1904 609 1925 preprint,
20 1 2 5 91 619 1903 879 1925 arxiv.org/abs/1710.03368,
20 1 2 6 96 888 1903 941 1920 2017.
21 1 1 1 93 301 1955 334 1975 He,
21 1 1 2 93 343 1955 371 1975 K.,
21 1 1 3 93 379 1955 447 1978 Zhang,
21 1 1 4 93 455 1955 484 1975 X.,
21 1 1 5 93 492 1955 537 1975 Ren,
21 1 1 6 92 546 1955 570 1975 S.,
21 1 1 7 96 578 1955 613 1972 and
21 1 1 8 97 621 1955 663 1975 Sun,
21 1 1 9 96 672 1955 685 1972 J.
21 1 1 10 96 698 1955 749 1978 Deep
21 1 1 11 96 756 1955 833 1972 residual
21 1 1 12 96 842 1955 922 1977 learning
21 1 1 13 96 930 1955 958 1972 for
21 1 1 14 96 965 1955 1025 1977 image
21 1 1 15 96 1032 1955 1150 1977 recognition.
21 1 1 16 95 1162 1955 1181 1972 In
21 1 1 17 95 1190 1955 1288 1977 Computer
21 1 1 18 96 1297 1956 1355 1972 Vision
21 1 1 19 96 1363 1955 1401 1972 and
21 1 2 1 96 328 1984 399 2000 Pattern
21 1 2 2 96 406 1984 526 2005 Recognition
21 1 2 3 96 534 1983 617 2004 (CVPR),
21 1 2 4 96 625 1983 678 2000 2016.
22 1 1 1 95 840 2064 863 2083 10
