block para line word confidence X1 Y1 X2 Y2 token
1 1 1 1 95 108 147 337 178 "   "
1 1 1 2 95 439 178 446 198 " "
1 1 2 1 95 108 178 446 380 "  "
2 1 1 1 95 332 149 676 152 " "
3 1 1 1 88 119 148 175 177 SetPoint’
3 1 1 2 85 716 160 759 172 Output
4 1 1 1 66 375 183 439 193 Controller
5 1 1 1 84 578 182 627 196 ‘System
6 1 1 1 95 479 150 480 224 " "
7 1 1 1 95 675 150 676 224 " "
8 1 1 1 95 528 150 529 225 " "
9 1 1 1 95 528 223 676 225 " "
10 1 1 1 95 442 333 591 337 " "
11 1 1 1 84 489 358 542 369 Sensors
11 1 2 1 96 484 376 507 386 and
11 1 2 2 95 513 375 552 386 Filters
12 1 1 1 95 589 334 590 409 " "
13 1 1 1 95 442 334 444 409 " "
14 1 1 1 95 442 406 591 410 " "
15 1 1 1 96 254 428 295 453 Fig.
15 1 1 2 96 308 428 326 447 3:
15 1 1 3 96 338 428 451 447 Controller
15 1 1 4 96 462 428 527 447 Block
15 1 1 5 96 538 428 633 453 Diagram
16 1 1 1 95 166 529 198 548 IV.
16 1 1 2 96 216 530 421 548 REINFORCEMENT
16 1 1 3 95 430 530 555 548 LEARNING
16 1 1 4 96 564 530 683 548 METHODS
16 1 1 5 95 693 533 721 548 AS
16 1 2 1 96 356 563 530 582 CONTROLLERS
17 1 1 1 95 132 609 153 627 In
17 1 1 2 95 171 608 206 631 [1],
17 1 1 3 96 224 614 254 627 we
17 1 1 4 95 270 608 351 627 worked
17 1 1 5 95 367 614 393 627 on
17 1 1 6 96 410 608 521 627 traditional
17 1 1 7 96 538 608 660 627 Controllers
17 1 1 8 96 677 608 716 627 like
17 1 1 9 96 733 608 782 631 PID,
17 1 2 1 93 104 642 170 666 Fuzzy
17 1 2 2 91 183 642 284 664 PD,PD+I
17 1 2 3 91 298 641 317 661 &
17 1 2 4 84 330 641 385 665 LQR
17 1 2 5 92 386 637 398 671 .
17 1 2 6 96 415 641 457 660 The
17 1 2 7 96 470 641 549 666 biggest
17 1 2 8 96 561 641 652 666 problem
17 1 2 9 96 665 641 713 660 with
17 1 2 10 96 726 641 783 660 those
17 1 3 1 96 104 674 196 693 methods
17 1 3 2 96 205 674 221 693 is
17 1 3 3 93 229 674 270 693 that
17 1 3 4 92 278 690 282 697 ,
17 1 3 5 96 291 674 336 699 they
17 1 3 6 95 344 674 395 693 need
17 1 3 7 96 403 677 423 693 to
17 1 3 8 97 431 674 456 693 be
17 1 3 9 96 464 674 524 693 tuned
17 1 3 10 96 532 674 637 699 manually.
17 1 3 11 96 647 674 680 697 So,
17 1 3 12 96 689 674 782 699 reaching
17 1 4 1 96 104 707 187 732 optimal
17 1 4 2 96 202 707 270 726 values
17 1 4 3 96 284 707 308 726 of
17 1 4 4 96 320 707 443 726 Controllers
17 1 4 5 96 458 707 547 732 depends
17 1 4 6 96 561 713 588 726 on
17 1 4 7 96 602 713 661 732 many
17 1 4 8 96 676 707 729 726 trials
17 1 4 9 96 745 707 783 726 and
17 1 5 1 96 104 747 173 760 errors.
17 1 5 2 96 186 742 248 766 Many
17 1 5 3 96 262 747 273 760 a
17 1 5 4 96 284 741 343 760 times
17 1 5 5 96 355 741 454 766 optimum
17 1 5 6 93 466 741 535 760 values
17 1 5 7 92 547 741 610 760 aren’t
17 1 5 8 97 622 741 707 760 reached
17 1 5 9 95 719 744 738 760 at
17 1 5 10 96 750 741 781 760 all.
17 1 6 1 96 104 774 146 793 The
17 1 6 2 96 161 774 240 799 biggest
17 1 6 3 96 255 774 330 793 benefit
17 1 6 4 95 345 774 369 793 of
17 1 6 5 96 382 774 546 793 Reinforcement
17 1 6 6 96 561 774 650 799 learning
17 1 6 7 95 666 774 782 799 algorithms
17 1 7 1 96 105 813 125 826 as
17 1 7 2 95 139 807 262 826 Controllers
17 1 7 3 96 276 807 293 826 is
17 1 7 4 93 307 807 347 826 that
17 1 7 5 89 361 823 365 830 ,
17 1 7 6 96 379 807 411 826 the
17 1 7 7 96 425 807 493 826 model
17 1 7 8 96 507 810 563 826 tunes
17 1 7 9 95 577 807 633 826 itself
17 1 7 10 96 645 811 665 826 to
17 1 7 11 96 678 807 737 826 reach
17 1 7 12 96 750 807 783 826 the
17 1 8 1 96 104 840 209 865 Optimum
17 1 8 2 96 227 840 301 859 values.
17 1 8 3 96 320 840 362 859 The
17 1 8 4 96 380 840 485 865 following
17 1 8 5 96 504 844 544 859 two
17 1 8 6 96 562 840 649 859 sections
17 1 8 7 93 667 840 746 859 discuss
17 1 8 8 93 764 840 783 864 Q
17 1 9 1 96 104 873 202 898 Learning
17 1 9 2 96 214 873 253 892 and
17 1 9 3 93 263 874 320 898 Deep
17 1 9 4 91 332 873 350 897 Q
17 1 9 5 96 361 873 462 892 Network.
17 2 1 1 91 102 908 124 926 A.
17 2 1 2 91 143 907 160 931 Q
17 2 1 3 96 171 908 272 932 Learning
18 1 1 1 92 132 951 159 975 Q-
18 1 1 2 96 172 951 262 976 learning
18 1 1 3 96 274 957 315 970 was
18 1 1 4 96 328 951 440 976 developed
18 1 1 5 96 452 951 478 976 by
18 1 1 6 96 491 951 621 976 Christopher
18 1 1 7 96 632 951 684 970 John
18 1 1 8 96 696 951 783 970 Cornish
18 1 2 1 92 104 984 190 1009 Hellaby
18 1 2 2 96 207 984 295 1003 Watkins
18 1 2 3 96 314 985 348 1007 [7].
18 1 2 4 96 366 984 481 1009 According
18 1 2 5 96 498 987 518 1003 to
18 1 2 6 96 535 984 629 1007 Watkins,
18 1 2 7 46 647 984 673 1003 “it
18 1 2 8 96 689 984 782 1009 provides
18 1 3 1 96 104 1020 173 1042 agents
18 1 3 2 96 189 1017 236 1036 with
18 1 3 3 96 251 1017 283 1036 the
18 1 3 4 96 298 1017 405 1042 capability
18 1 3 5 96 421 1017 445 1036 of
18 1 3 6 96 458 1017 547 1042 learning
18 1 3 7 96 562 1020 582 1036 to
18 1 3 8 96 597 1021 629 1036 act
18 1 3 9 95 643 1017 747 1042 optimally
18 1 3 10 96 763 1017 783 1036 in
18 1 4 1 96 104 1050 223 1069 Markovian
18 1 4 2 96 239 1050 331 1069 domains
18 1 4 3 96 347 1050 373 1075 by
18 1 4 4 96 389 1050 531 1075 experiencing
18 1 4 5 96 547 1050 579 1069 the
18 1 4 6 96 594 1056 745 1075 consequences
18 1 4 7 96 761 1050 785 1069 of
18 1 5 1 96 104 1084 187 1107 actions,
18 1 5 2 96 204 1084 287 1103 without
18 1 5 3 96 301 1084 401 1109 requiring
18 1 5 4 96 417 1084 471 1103 them
18 1 5 5 96 486 1088 506 1103 to
18 1 5 6 96 520 1084 576 1103 build
18 1 5 7 96 591 1090 647 1109 maps
18 1 5 8 96 663 1084 687 1103 of
18 1 5 9 93 700 1084 732 1103 the
18 1 5 10 91 747 1084 782 1103 do-
18 1 6 1 95 104 1117 186 1136 mains.”
18 1 6 2 95 199 1117 233 1140 [8].
18 1 6 3 92 246 1117 267 1136 In
18 1 6 4 92 284 1123 289 1136 a
18 1 6 5 96 300 1117 419 1136 Markovian
18 1 6 6 93 430 1117 518 1140 domain,
18 1 6 7 91 530 1116 549 1141 Q
18 1 6 8 92 561 1117 660 1136 function-
18 1 6 9 96 672 1117 704 1136 the
18 1 6 10 96 715 1117 783 1136 model
18 1 7 1 96 104 1153 124 1169 to
18 1 7 2 97 134 1150 159 1169 be
18 1 7 3 96 170 1150 277 1175 generated
18 1 7 4 96 287 1150 345 1175 using
18 1 7 5 93 356 1150 388 1169 the
18 1 7 6 92 399 1150 514 1175 algorithm-
18 1 7 7 96 525 1150 632 1169 calculates
18 1 7 8 96 643 1150 675 1169 the
18 1 7 9 96 686 1150 783 1175 expected
18 1 8 1 96 104 1183 168 1208 utility
18 1 8 2 96 184 1183 216 1202 for
18 1 8 3 96 231 1189 242 1202 a
18 1 8 4 96 257 1183 316 1208 given
18 1 8 5 96 332 1183 387 1202 finite
18 1 8 6 93 403 1186 451 1202 state
18 1 8 7 92 468 1190 478 1202 s
18 1 8 8 96 494 1183 533 1202 and
18 1 8 9 96 548 1189 607 1208 every
18 1 8 10 96 622 1183 712 1208 possible
18 1 8 11 96 728 1183 783 1202 finite
18 1 9 1 96 104 1217 170 1236 action
18 1 9 2 96 181 1224 200 1236 a.
18 1 9 3 96 211 1217 253 1236 The
18 1 9 4 93 264 1221 322 1242 agent
18 1 9 5 93 332 1229 340 1231 -
18 1 9 6 96 351 1217 417 1236 which
18 1 9 7 96 427 1217 444 1236 is
18 1 9 8 96 455 1217 487 1236 the
18 1 9 9 96 497 1217 555 1236 robot
18 1 9 10 96 565 1217 585 1236 in
18 1 9 11 93 596 1217 634 1236 this
18 1 9 12 92 644 1223 700 1236 case-
18 1 9 13 96 711 1217 782 1236 selects
18 1 10 1 96 104 1250 136 1269 the
18 1 10 2 96 148 1250 247 1275 optimum
18 1 10 3 96 259 1250 325 1269 action
18 1 10 4 96 337 1257 350 1269 a
18 1 10 5 96 361 1250 435 1275 having
18 1 10 6 96 447 1250 479 1269 the
18 1 10 7 96 490 1250 569 1275 highest
18 1 10 8 96 581 1250 638 1269 value
18 1 10 9 93 650 1250 674 1269 of
18 1 10 10 84 684 1248 764 1276 Q(s,a)
18 1 10 11 90 778 1266 782 1273 ,
18 1 11 1 96 104 1283 142 1302 this
18 1 11 2 96 154 1283 220 1302 action
18 1 11 3 96 230 1283 329 1308 choosing
18 1 11 4 96 339 1283 381 1302 rule
18 1 11 5 96 392 1283 408 1302 is
18 1 11 6 96 420 1283 463 1302 also
18 1 11 7 95 473 1283 538 1302 called
18 1 11 8 96 549 1283 622 1308 Policy.
18 1 11 9 93 635 1283 663 1306 [8]
18 1 11 10 92 677 1299 679 1302 .
18 1 11 11 95 691 1283 782 1308 Initially,
18 1 12 1 93 104 1316 136 1335 the
18 1 12 2 87 151 1315 230 1342 Q(s,a)
18 1 12 3 96 246 1316 337 1335 function
18 1 12 4 96 351 1316 419 1335 values
18 1 12 5 96 433 1322 465 1335 are
18 1 12 6 96 479 1316 573 1335 assumed
18 1 12 7 96 587 1319 607 1335 to
18 1 12 8 96 620 1316 645 1335 be
18 1 12 9 96 659 1322 711 1335 zero.
18 1 12 10 97 726 1316 783 1335 After
18 1 13 1 96 104 1355 162 1374 every
18 1 13 2 96 174 1349 258 1374 training
18 1 13 3 90 270 1353 312 1374 step
18 1 13 4 90 324 1365 328 1372 ,
18 1 13 5 96 339 1349 371 1368 the
18 1 13 6 96 383 1349 450 1368 values
18 1 13 7 96 461 1355 493 1368 are
18 1 13 8 96 504 1349 591 1374 updated
18 1 13 9 96 601 1349 709 1374 according
18 1 13 10 96 720 1352 740 1368 to
18 1 13 11 96 751 1349 783 1368 the
18 1 14 1 96 104 1383 210 1408 following
18 1 14 2 96 221 1383 315 1408 equation
19 1 1 1 82 174 1469 224 1497 Q(s,
19 1 1 2 53 232 1470 263 1497 a4)
19 1 1 3 88 276 1476 300 1490 —
19 1 1 4 52 310 1469 360 1497 Q(s,
19 1 1 5 80 368 1470 399 1497 a4)
19 1 1 6 74 410 1474 428 1492 +
19 1 1 7 74 437 1469 477 1497 a(r
19 1 1 8 81 486 1474 503 1492 +
19 1 1 9 17 512 1469 711 1497 ymarQ(se41,4))
19 1 1 10 0 752 1472 782 1495 " ()"
20 1 1 1 96 131 1558 173 1577 The
20 1 1 2 96 189 1558 288 1583 objective
20 1 1 3 96 304 1558 335 1577 for
20 1 1 4 96 351 1558 383 1577 the
20 1 1 5 96 399 1558 469 1577 Model
20 1 1 6 96 485 1558 505 1577 in
20 1 1 7 96 521 1564 557 1577 our
20 1 1 8 96 572 1558 648 1583 project
20 1 1 9 96 664 1558 680 1577 is
20 1 1 10 96 696 1561 716 1577 to
20 1 1 11 96 732 1558 783 1583 keep
20 1 2 1 96 104 1592 119 1611 it
20 1 2 2 96 134 1592 203 1611 within
20 1 2 3 95 219 1592 281 1611 limits
20 1 2 4 92 297 1592 328 1611 ie.
20 1 2 5 92 346 1588 390 1612 +5°
20 1 2 6 86 409 1608 412 1611 .
20 1 2 7 96 429 1593 456 1611 At
20 1 2 8 92 472 1592 513 1611 first
20 1 2 9 91 530 1608 534 1614 ,
20 1 2 10 96 550 1592 583 1611 the
20 1 2 11 96 599 1592 656 1611 robot
20 1 2 12 96 672 1592 745 1615 model,
20 1 2 13 90 763 1591 782 1616 Q
20 1 3 1 96 104 1625 181 1648 matrix,
20 1 3 2 96 199 1625 268 1650 Policy
20 1 3 3 46 286 1632 301 1644 a
20 1 3 4 96 320 1631 351 1644 are
20 1 3 5 93 369 1625 479 1644 initialized
20 1 3 6 92 497 1641 501 1644 .
20 1 3 7 96 519 1625 582 1644 There
20 1 3 8 96 600 1631 632 1644 are
20 1 3 9 91 651 1631 707 1644 some
20 1 3 10 91 725 1625 782 1644 inter-
20 1 4 1 96 104 1658 169 1683 esting
20 1 4 2 96 186 1658 252 1683 points
20 1 4 3 96 270 1661 290 1677 to
20 1 4 4 96 308 1658 372 1677 make.
20 1 4 5 96 390 1658 432 1677 The
20 1 4 6 96 450 1661 509 1677 states
20 1 4 7 96 527 1664 559 1677 are
20 1 4 8 96 577 1661 612 1677 not
20 1 4 9 95 629 1658 690 1677 finite.
20 1 4 10 96 709 1658 783 1677 Within
20 1 5 1 96 104 1691 156 1710 limit
20 1 5 2 93 170 1697 230 1716 range
20 1 5 3 92 246 1707 250 1714 ,
20 1 5 4 96 266 1691 366 1710 hundreds
20 1 5 5 95 382 1691 420 1710 and
20 1 5 6 96 435 1691 544 1710 thousands
20 1 5 7 96 560 1691 584 1710 of
20 1 5 8 96 596 1691 651 1716 pitch
20 1 5 9 96 666 1691 735 1716 angles
20 1 5 10 96 751 1697 783 1710 are
20 1 6 1 95 104 1725 199 1750 possible.
20 1 6 2 96 219 1725 299 1750 Having
20 1 6 3 96 318 1725 427 1744 thousands
20 1 6 4 96 446 1725 469 1744 of
20 1 6 5 95 486 1725 578 1744 columns
20 1 6 6 95 598 1725 615 1744 is
20 1 6 7 95 634 1728 668 1744 not
20 1 6 8 95 686 1725 781 1750 possible.
20 1 7 1 96 105 1758 138 1781 So,
20 1 7 2 96 155 1758 187 1777 the
20 1 7 3 96 204 1761 253 1777 state
20 1 7 4 96 270 1758 338 1777 values
20 1 7 5 96 354 1764 406 1777 were
20 1 7 6 96 423 1758 547 1777 discretized.
20 1 7 7 96 565 1759 599 1777 We
20 1 7 8 96 615 1758 734 1777 discretized
20 1 7 9 96 751 1758 783 1777 the
20 1 8 1 95 104 1791 172 1810 values
20 1 8 2 95 193 1794 213 1810 to
20 1 8 3 96 233 1791 260 1810 20
20 1 8 4 96 280 1791 364 1810 discrete
20 1 8 5 96 385 1794 434 1810 state
20 1 8 6 96 454 1791 523 1816 angles
20 1 8 7 92 544 1791 596 1810 from
20 1 8 8 90 618 1787 675 1811 —10°
20 1 8 9 96 697 1795 717 1810 to
20 1 8 10 95 739 1787 781 1811 10°.
20 1 9 1 96 104 1825 142 1843 For
20 1 9 2 96 158 1824 224 1843 action
20 1 9 3 96 241 1824 304 1847 value,
20 1 9 4 96 322 1830 352 1843 we
20 1 9 5 96 369 1824 430 1843 chose
20 1 9 6 95 450 1824 474 1843 10
20 1 9 7 95 490 1824 584 1843 different
20 1 9 8 95 600 1824 710 1843 velocities.
20 1 9 9 96 727 1824 782 1849 They
20 1 10 1 93 104 1863 136 1876 are
20 1 10 2 64 147 1855 220 1883 [—200,
20 1 10 3 82 230 1858 295 1881 —100,
20 1 10 4 85 304 1858 357 1881 —50,
20 1 10 5 86 366 1858 418 1881 —25,
20 1 10 6 75 428 1858 480 1881 —10,
20 1 10 7 96 489 1858 520 1881 10,
20 1 10 8 91 528 1858 560 1881 25,
20 1 10 9 91 568 1858 600 1881 50,
20 1 10 10 93 609 1858 654 1881 100,
20 1 10 11 0 662 1853 782 1883 200]ms~t.
20 1 11 1 93 104 1891 146 1910 The
20 1 11 2 88 159 1890 179 1916 Q
20 1 11 3 96 192 1891 263 1910 matrix
20 1 11 4 96 276 1891 315 1910 had
20 1 11 5 96 328 1891 354 1910 20
20 1 11 6 93 367 1891 459 1910 columns
20 1 11 7 93 473 1907 477 1914 ,
20 1 11 8 96 490 1891 540 1910 each
20 1 11 9 96 553 1891 634 1910 column
20 1 11 10 96 647 1891 783 1916 representing
20 1 12 1 96 104 1930 116 1943 a
20 1 12 2 96 127 1928 175 1943 state
20 1 12 3 96 186 1924 225 1943 and
20 1 12 4 96 239 1924 262 1943 10
20 1 12 5 96 273 1930 324 1943 rows
20 1 12 6 96 336 1924 385 1943 each
20 1 12 7 96 396 1924 531 1949 representing
20 1 12 8 97 542 1930 601 1949 every
20 1 12 9 96 613 1924 684 1943 action.
20 1 12 10 96 697 1924 782 1949 Initially
20 1 13 1 0 105 1957 143 1980 the
20 1 13 2 90 158 1957 176 1981 Q
20 1 13 3 92 190 1957 268 1976 -values
20 1 13 4 96 282 1963 334 1976 were
20 1 13 5 96 349 1957 443 1976 assumed
20 1 13 6 96 457 1960 477 1976 to
20 1 13 7 96 490 1957 515 1976 be
20 1 13 8 96 530 1958 542 1977 0
20 1 13 9 96 556 1957 595 1976 and
20 1 13 10 96 608 1957 692 1976 random
20 1 13 11 95 706 1957 783 1976 actions
20 1 14 1 96 105 1996 156 2009 were
20 1 14 2 96 168 1990 265 2015 specified
20 1 14 3 97 276 1990 307 2009 for
20 1 14 4 96 318 1996 376 2015 every
20 1 14 5 96 389 1994 437 2009 state
20 1 14 6 96 448 1990 469 2009 in
20 1 14 7 97 480 1990 512 2009 the
20 1 14 8 96 523 1990 591 2015 policy
20 1 14 9 73 602 1997 617 2009 7
20 1 14 10 90 630 2006 633 2009 .
20 1 14 11 96 645 1990 687 2009 The
20 1 14 12 96 698 1990 782 2015 training
20 1 15 1 96 104 2029 145 2042 was
20 1 15 2 95 154 2023 207 2042 done
20 1 15 3 95 216 2023 248 2042 for
20 1 15 4 96 259 2023 310 2043 1500
20 1 15 5 96 319 2023 413 2048 episodes
20 1 15 6 96 422 2023 461 2042 and
20 1 15 7 96 470 2023 490 2042 in
20 1 15 8 97 499 2023 549 2042 each
20 1 15 9 96 557 2023 647 2048 episode,
20 1 15 10 97 657 2023 689 2042 the
20 1 15 11 96 698 2023 782 2048 training
21 1 1 1 95 352 1593 357 1710 " "
22 1 1 1 96 938 179 1017 195 Rewards
22 1 1 2 96 1025 183 1045 195 vs
22 1 1 3 96 1053 179 1135 199 Episodes
22 1 1 4 96 1142 179 1168 195 for
22 1 1 5 96 1176 178 1258 195 Different
22 1 1 6 96 1266 180 1346 199 Learning
22 1 1 7 95 1355 180 1406 195 Rates
23 1 1 1 95 908 203 1437 207 " "
24 1 1 1 95 927 213 1417 466 " "
25 1 1 1 84 1199 476 1232 479 —
25 1 1 2 51 1245 469 1293 483 Ir0.7,
25 1 1 3 74 1300 469 1360 482 Gamma
25 1 1 4 89 1367 469 1410 482 0.999
26 1 1 1 74 866 486 895 518 800
26 1 1 2 86 1200 500 1232 502 ——
26 1 1 3 0 1245 493 1254 506 Ir
26 1 1 4 0 1261 493 1293 506 0.65,
26 1 1 5 85 1315 489 1365 515 Gamma
26 1 1 6 94 1375 489 1420 515 0.999
26 1 2 1 83 1199 524 1232 526 ——
26 1 2 2 17 1245 517 1360 531 |Ir0.8,Gamma
26 1 2 3 96 1367 517 1410 530 0.999
27 1 1 1 95 908 204 909 548 " "
28 1 1 1 95 1436 204 1438 549 " "
29 1 1 1 95 908 546 1438 549 " "
30 1 1 1 60 929 558 938 571 d
30 1 1 2 58 983 558 1009 571 200
30 1 1 3 14 1046 558 1072 571 «400
30 1 1 4 14 1111 548 1201 571 «600-80
30 1 1 5 13 1234 548 1333 571 1000-1200
30 1 1 6 79 1362 558 1397 571 1400
30 1 2 1 90 1138 580 1206 596 Episodes
31 1 1 1 96 987 624 1028 649 Fig.
31 1 1 2 96 1040 624 1059 643 4:
31 1 1 3 96 1072 624 1166 643 Rewards
31 1 1 4 97 1177 624 1209 643 for
31 1 1 5 96 1219 624 1313 643 different
31 1 1 6 96 1324 631 1339 643 a
32 1 1 1 96 824 727 865 740 was
32 1 1 2 96 877 721 958 740 iterated
32 1 1 3 96 970 722 1023 741 2000
32 1 1 4 93 1034 721 1092 740 times
32 1 1 5 91 1104 737 1107 740 .
32 1 1 6 97 1120 722 1146 740 At
32 1 1 7 97 1157 721 1189 740 the
32 1 1 8 96 1200 721 1309 746 beginning
32 1 1 9 96 1320 721 1344 740 of
32 1 1 10 96 1352 721 1402 740 each
32 1 1 11 96 1413 721 1502 746 episode,
32 1 2 1 96 824 755 856 774 the
32 1 2 2 96 873 755 988 774 simulation
32 1 2 3 96 1005 761 1045 774 was
32 1 2 4 95 1061 755 1169 774 refreshed.
32 1 2 5 95 1187 755 1298 774 Whenever
32 1 2 6 93 1314 755 1346 774 the
32 1 2 7 92 1361 755 1437 774 robot’s
32 1 2 8 96 1454 758 1503 774 state
32 1 3 1 96 824 788 925 807 exceeded
32 1 3 2 96 938 788 970 807 the
32 1 3 3 96 982 788 1033 807 limit
32 1 3 4 96 1045 788 1060 807 it
32 1 3 5 96 1072 794 1112 807 was
32 1 3 6 96 1125 788 1230 813 penalized
32 1 3 7 96 1242 788 1268 813 by
32 1 3 8 96 1281 788 1384 813 assigning
32 1 3 9 96 1396 788 1471 807 reward
32 1 3 10 96 1483 792 1503 807 to
32 1 4 1 76 826 821 885 841 —100
32 1 4 2 92 901 837 903 840 .
32 1 4 3 93 918 821 960 840 The
32 1 4 4 91 974 820 993 845 Q
32 1 4 5 96 1007 821 1066 840 Table
32 1 4 6 97 1080 821 1097 840 is
32 1 4 7 96 1111 821 1197 846 updated
32 1 4 8 97 1210 824 1229 840 at
32 1 4 9 96 1242 821 1291 840 each
32 1 4 10 96 1306 825 1348 846 step
32 1 4 11 96 1362 821 1469 846 according
32 1 4 12 96 1483 825 1503 840 to
32 1 5 1 96 824 854 918 879 equation
32 1 5 2 91 932 854 947 873 1.
32 1 5 3 96 959 854 1001 873 The
32 1 5 4 96 1012 854 1126 879 Algorithm
32 1 5 5 91 1140 854 1147 873 1
32 1 5 6 96 1161 854 1227 873 shows
32 1 5 7 96 1239 854 1271 873 the
32 1 5 8 96 1283 854 1319 873 full
32 1 5 9 96 1331 854 1442 879 algorithm.
32 2 1 1 96 851 887 893 906 The
32 2 1 2 95 913 887 1028 906 simulation
32 2 1 3 96 1048 893 1088 906 was
32 2 1 4 96 1108 893 1143 906 run
32 2 1 5 96 1163 887 1194 906 for
32 2 1 6 96 1213 887 1267 906 three
32 2 1 7 96 1286 887 1380 906 different
32 2 1 8 95 1399 894 1414 906 a
32 2 1 9 96 1434 887 1502 906 values
32 2 2 1 73 826 919 875 945 (0.7,
32 2 2 2 3 886 921 937 944 0.65,
32 2 2 3 3 944 919 987 945 0.8)
32 2 2 4 92 1001 936 1005 943 ,
32 2 2 5 96 1017 920 1065 939 with
32 2 2 6 64 1076 927 1090 944 7
32 2 2 7 96 1102 920 1159 939 value
32 2 2 8 96 1171 920 1195 939 of
32 2 2 9 96 1206 919 1293 946 (0.999).
32 2 2 10 96 1305 920 1347 939 The
32 2 2 11 96 1358 920 1399 945 Fig.
32 2 2 12 96 1411 920 1424 939 4
32 2 2 13 96 1436 920 1502 939 shows
32 2 3 1 96 824 954 856 973 the
32 2 3 2 96 867 954 961 973 Rewards
32 2 3 3 96 972 960 995 973 vs
32 2 3 4 96 1006 954 1104 979 Episodes
32 2 3 5 96 1115 954 1147 973 for
32 2 3 6 96 1157 954 1213 973 those
32 2 3 7 96 1225 960 1257 973 as.
32 2 3 8 96 1269 955 1285 973 It
32 2 3 9 96 1295 954 1312 973 is
32 2 3 10 96 1323 954 1402 973 evident
32 2 3 11 96 1413 954 1459 977 that,
32 2 3 12 96 1470 954 1502 973 the
32 2 4 1 93 824 987 882 1006 robot
32 2 4 2 91 894 987 985 1006 couldn’t
32 2 4 3 96 998 987 1057 1006 reach
32 2 4 4 96 1070 987 1103 1006 the
32 2 4 5 96 1116 991 1178 1012 target
32 2 4 6 96 1191 987 1276 1006 rewards
32 2 4 7 97 1290 987 1359 1006 within
32 2 4 8 97 1372 987 1404 1006 the
32 2 4 9 96 1418 987 1503 1012 training
32 2 5 1 96 824 1020 894 1045 period
32 2 5 2 96 912 1020 943 1039 for
32 2 5 3 96 960 1020 1017 1039 those
32 2 5 4 96 1035 1020 1124 1045 learning
32 2 5 5 96 1142 1024 1199 1039 rates.
32 2 5 6 96 1218 1021 1252 1039 We
32 2 5 7 96 1271 1026 1304 1039 see
32 2 5 8 96 1323 1020 1369 1043 that,
32 2 5 9 96 1387 1020 1419 1039 for
32 2 5 10 96 1436 1020 1468 1039 the
32 2 5 11 96 1487 1027 1502 1039 a
32 2 6 1 96 825 1053 892 1072 values
32 2 6 2 96 909 1053 942 1073 0.7
32 2 6 3 96 958 1053 997 1072 and
32 2 6 4 96 1013 1054 1052 1076 0.8,
32 2 6 5 96 1069 1053 1101 1072 the
32 2 6 6 96 1117 1053 1174 1072 robot
32 2 6 7 96 1189 1053 1271 1072 reaches
32 2 6 8 95 1287 1053 1398 1072 maximum
32 2 6 9 95 1413 1053 1503 1078 possible
32 2 7 1 96 824 1086 963 1105 accumulated
32 2 7 2 96 975 1086 1067 1109 rewards,
32 2 7 3 96 1080 1086 1126 1109 200,
32 2 7 4 96 1140 1086 1209 1105 within
32 2 7 5 96 1222 1086 1262 1105 400
32 2 7 6 96 1275 1086 1375 1111 episodes.
32 2 7 7 95 1388 1086 1430 1105 The
32 2 7 8 96 1443 1092 1503 1105 curve
32 2 8 1 96 824 1120 872 1139 with
32 2 8 2 75 883 1127 899 1139 a
32 2 8 3 96 911 1120 968 1139 value
32 2 8 4 96 980 1120 1014 1140 0.7
32 2 8 5 96 1025 1120 1041 1139 is
32 2 8 6 96 1053 1120 1092 1139 less
32 2 8 7 96 1105 1120 1167 1139 stable
32 2 8 8 95 1178 1120 1286 1145 compared
32 2 8 9 95 1297 1123 1317 1139 to
32 2 8 10 96 1328 1120 1369 1139 that
32 2 8 11 96 1379 1120 1403 1139 of
32 2 8 12 96 1412 1121 1452 1140 0.8.
32 2 8 13 96 1464 1121 1503 1139 But
32 2 9 1 96 824 1153 866 1172 The
32 2 9 2 96 879 1159 939 1172 curve
32 2 9 3 96 953 1153 1001 1172 with
32 2 9 4 75 1015 1160 1030 1172 a
32 2 9 5 96 1045 1153 1102 1172 value
32 2 9 6 96 1116 1153 1163 1173 0.65
32 2 9 7 96 1178 1159 1237 1172 never
32 2 9 8 96 1250 1153 1332 1172 reaches
32 2 9 9 96 1346 1153 1378 1172 the
32 2 9 10 96 1392 1153 1503 1172 maximum
32 2 10 1 96 824 1186 963 1205 accumulated
32 2 10 2 96 973 1186 1054 1205 reward.
33 1 1 1 90 824 1238 844 1256 B.
33 1 1 2 92 861 1238 919 1262 Deep
33 1 1 3 92 931 1237 949 1261 Q
33 1 1 4 93 958 1237 1053 1256 Network
33 1 1 5 89 1063 1238 1138 1261 (DQN)
34 1 1 1 89 852 1280 869 1298 V
34 1 1 2 90 884 1279 943 1298 Mnih
34 1 1 3 96 956 1282 976 1298 et
34 1 1 4 96 989 1279 1007 1298 al
34 1 1 5 96 1023 1279 1051 1302 [9]
34 1 1 6 96 1066 1279 1108 1298 first
34 1 1 7 96 1122 1279 1171 1298 used
34 1 1 8 96 1185 1280 1242 1304 Deep
34 1 1 9 96 1255 1279 1354 1304 Learning
34 1 1 10 95 1368 1285 1389 1298 as
34 1 1 11 95 1398 1275 1409 1308 a
34 1 1 12 95 1428 1279 1503 1298 variant
34 1 2 1 92 824 1312 848 1331 of
34 1 2 2 92 858 1312 876 1336 Q
34 1 2 3 96 888 1312 986 1337 Learning
34 1 2 4 95 999 1312 1105 1337 algorithm
34 1 2 5 95 1117 1315 1137 1331 to
34 1 2 6 96 1148 1312 1194 1337 play
34 1 2 7 96 1207 1312 1238 1331 six
34 1 2 8 96 1249 1318 1318 1337 games
34 1 2 9 96 1330 1312 1354 1331 of
34 1 2 10 96 1364 1312 1419 1331 Atari
34 1 2 11 93 1432 1312 1485 1331 2600
34 1 2 12 93 1498 1328 1502 1335 ,
34 1 3 1 96 824 1345 890 1364 which
34 1 3 2 96 901 1345 1051 1370 outperformed
34 1 3 3 95 1062 1345 1088 1364 all
34 1 3 4 97 1099 1345 1155 1364 other
34 1 3 5 96 1165 1345 1258 1370 previous
34 1 3 6 93 1270 1345 1392 1370 algorithms.
34 1 3 7 90 1406 1361 1408 1364 .
34 1 3 8 96 1421 1346 1442 1364 In
34 1 3 9 96 1454 1345 1503 1364 their
34 1 4 1 96 824 1384 889 1403 paper,
34 1 4 2 96 901 1381 941 1397 two
34 1 4 3 96 952 1378 1026 1403 unique
34 1 4 4 96 1037 1378 1160 1403 approaches
34 1 4 5 96 1172 1384 1223 1397 were
34 1 4 6 96 1235 1378 1290 1397 used.
34 2 1 1 81 852 1421 861 1429 «
34 2 1 2 96 880 1411 1003 1436 Experience
34 2 1 3 96 1014 1411 1090 1436 Replay
34 3 1 1 16 852 1455 861 1463 e
34 3 1 2 96 880 1445 995 1464 Derivation
34 3 1 3 93 1006 1445 1030 1464 of
34 3 1 4 93 1039 1445 1058 1469 Q
34 3 1 5 96 1069 1445 1141 1464 Values
34 3 1 6 96 1152 1445 1172 1464 in
34 3 1 7 96 1183 1451 1222 1464 one
34 3 1 8 96 1233 1445 1319 1464 forward
34 3 1 9 96 1330 1451 1376 1470 pass
35 1 1 1 96 851 1516 893 1535 The
35 1 1 2 96 908 1516 1014 1541 technique
35 1 1 3 96 1028 1516 1052 1535 of
35 1 1 4 96 1064 1516 1187 1541 Experience
35 1 1 5 96 1202 1516 1283 1541 Replay,
35 1 1 6 95 1299 1516 1427 1541 experiences
35 1 1 7 96 1442 1516 1465 1535 of
35 1 1 8 96 1478 1522 1503 1535 an
35 1 2 1 92 824 1552 883 1574 agent
35 1 2 2 92 892 1565 896 1572 ,
35 1 2 3 83 905 1549 936 1568 i.e.
35 1 2 4 95 948 1548 1022 1575 (state,
35 1 2 5 94 1030 1549 1124 1573 reward,
35 1 2 6 93 1132 1550 1212 1573 action,
35 1 2 7 56 1220 1548 1330 1575 statenew)
35 1 2 8 96 1340 1555 1372 1568 are
35 1 2 9 96 1382 1549 1447 1568 stored
35 1 2 10 96 1456 1555 1503 1568 over
35 1 3 1 96 824 1588 883 1607 many
35 1 3 2 93 897 1582 990 1607 episodes
35 1 3 3 92 1005 1598 1008 1601 .
35 1 3 4 96 1023 1583 1044 1601 In
35 1 3 5 96 1058 1582 1090 1601 the
35 1 3 6 96 1103 1582 1192 1607 learning
35 1 3 7 96 1205 1582 1281 1607 period,
35 1 3 8 95 1295 1582 1345 1601 after
35 1 3 9 95 1357 1582 1407 1601 each
35 1 3 10 96 1420 1582 1503 1607 episode
35 1 4 1 96 824 1615 908 1634 random
35 1 4 2 96 918 1615 1000 1634 batches
35 1 4 3 96 1012 1615 1035 1634 of
35 1 4 4 96 1045 1615 1090 1634 data
35 1 4 5 96 1101 1615 1154 1634 from
35 1 4 6 96 1165 1615 1283 1640 experience
35 1 4 7 96 1294 1621 1326 1634 are
35 1 4 8 96 1339 1615 1387 1634 used
35 1 4 9 96 1399 1619 1419 1634 to
35 1 4 10 95 1431 1615 1503 1640 update
35 1 5 1 96 824 1648 856 1667 the
35 1 5 2 96 868 1648 941 1667 model.
35 1 5 3 96 955 1648 990 1671 [9].
35 1 5 4 96 1002 1648 1065 1667 There
35 1 5 5 97 1077 1654 1109 1667 are
35 1 5 6 96 1121 1648 1195 1667 several
35 1 5 7 96 1207 1648 1291 1667 benefits
35 1 5 8 96 1303 1648 1326 1667 of
35 1 5 9 97 1336 1648 1385 1667 such
35 1 5 10 96 1396 1648 1501 1673 approach.
35 1 6 1 96 824 1682 940 1707 According
35 1 6 2 96 951 1686 971 1701 to
35 1 6 3 96 982 1682 1014 1701 the
35 1 6 4 96 1025 1688 1090 1707 paper,
35 2 1 1 55 852 1730 861 1738 «
35 2 1 2 96 880 1721 896 1739 It
35 2 1 3 96 906 1720 976 1739 allows
35 2 1 4 96 987 1723 1064 1745 greater
35 2 1 5 96 1073 1720 1119 1739 data
35 2 1 6 96 1129 1720 1235 1745 efficiency
35 2 1 7 96 1247 1726 1268 1739 as
35 2 1 8 96 1279 1720 1329 1739 each
35 2 1 9 96 1340 1723 1383 1745 step
35 2 1 10 93 1394 1720 1417 1739 of
35 2 1 11 93 1426 1720 1502 1745 experi-
35 3 1 1 96 880 1759 929 1772 ence
35 3 1 2 96 940 1759 977 1772 can
35 3 1 3 95 988 1753 1013 1772 be
35 3 1 4 96 1024 1753 1074 1772 used
35 3 1 5 96 1085 1753 1105 1772 in
35 3 1 6 96 1116 1759 1175 1778 many
35 3 1 7 96 1187 1753 1261 1778 weight
35 3 1 8 96 1271 1753 1354 1778 updates
35 4 1 1 60 852 1796 861 1804 «
35 4 1 2 91 880 1786 1027 1811 Randomizing
35 4 1 3 96 1037 1786 1119 1805 batches
35 4 1 4 96 1129 1786 1200 1805 breaks
35 4 1 5 96 1210 1786 1339 1805 correlations
35 4 1 6 92 1349 1786 1440 1805 between
35 4 1 7 92 1451 1792 1502 1805 sam-
35 5 1 1 96 879 1820 922 1845 ples
35 6 1 1 31 852 1863 861 1871 «
35 6 1 2 96 880 1853 994 1872 Behaviour
35 6 1 3 95 1010 1853 1135 1872 distribution
35 6 1 4 96 1152 1853 1169 1872 is
35 6 1 5 96 1186 1853 1283 1878 averaged
35 6 1 6 96 1300 1859 1347 1872 over
35 6 1 7 96 1363 1859 1423 1878 many
35 6 1 8 95 1440 1853 1463 1872 of
35 6 1 9 95 1478 1853 1503 1872 its
35 7 1 1 96 879 1886 973 1911 previous
35 7 1 2 95 985 1890 1044 1905 states
35 7 2 1 96 824 1925 846 1943 In
35 7 2 2 96 859 1924 891 1943 the
35 7 2 3 92 904 1924 996 1943 classical
35 7 2 4 92 1009 1924 1028 1948 Q
35 7 2 5 95 1041 1924 1130 1949 learning
35 7 2 6 96 1143 1924 1249 1949 approach,
35 7 2 7 96 1263 1930 1301 1943 one
35 7 2 8 96 1314 1924 1349 1943 has
35 7 2 9 96 1363 1927 1382 1943 to
35 7 2 10 96 1395 1924 1440 1949 give
35 7 2 11 96 1454 1927 1503 1943 state
35 7 3 1 96 824 1957 863 1976 and
35 7 3 2 96 877 1957 943 1976 action
35 7 3 3 96 958 1963 979 1976 as
35 7 3 4 96 994 1963 1018 1976 an
35 7 3 5 96 1033 1957 1089 1982 input
35 7 3 6 96 1102 1957 1198 1982 resulting
35 7 3 7 93 1212 1957 1232 1976 in
35 7 3 8 80 1247 1956 1267 1981 Q
35 7 3 9 96 1282 1957 1339 1976 value
35 7 3 10 96 1354 1957 1385 1976 for
35 7 3 11 96 1399 1957 1440 1976 that
35 7 3 12 96 1454 1960 1502 1976 state
35 7 4 1 96 824 1990 863 2009 and
35 7 4 2 96 875 1990 946 2009 action.
35 7 4 3 96 960 1990 1086 2015 Replicating
35 7 4 4 96 1098 1990 1136 2009 this
35 7 4 5 96 1149 1990 1249 2015 approach
35 7 4 6 96 1261 1990 1281 2009 in
35 7 4 7 96 1293 1990 1366 2009 Neural
35 7 4 8 96 1378 1990 1474 2009 Network
35 7 4 9 96 1486 1990 1502 2009 is
35 7 5 1 96 824 2023 960 2048 problematic.
35 7 5 2 96 974 2024 1064 2042 Because
35 7 5 3 96 1076 2023 1096 2042 in
35 7 5 4 96 1108 2023 1148 2042 that
35 7 5 5 96 1159 2029 1205 2042 case
35 7 5 6 96 1217 2029 1256 2042 one
35 7 5 7 95 1267 2023 1303 2042 has
35 7 5 8 96 1315 2027 1335 2042 to
35 7 5 9 96 1347 2023 1391 2048 give
35 7 5 10 97 1404 2026 1452 2042 state
35 7 5 11 96 1465 2023 1503 2042 and
