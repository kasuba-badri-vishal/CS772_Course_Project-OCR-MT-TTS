block para line word confidence X1 Y1 X2 Y2 token
1 1 1 1 95 202 174 805 175 " "
2 1 1 1 95 202 176 805 177 " "
3 1 1 1 95 202 228 805 229 " "
4 1 1 1 96 302 247 320 264 et
5 1 1 1 95 202 356 805 358 " "
6 1 1 1 96 302 375 323 392 et
7 1 1 1 95 202 485 805 487 " "
8 1 1 1 96 200 521 260 540 Table
8 1 1 2 96 271 522 287 540 1:
8 1 1 3 95 300 520 418 547 Language
8 1 1 4 96 427 519 540 547 modeling
8 1 1 5 96 549 519 584 540 for
8 1 1 6 96 592 520 645 540 PTB
8 1 2 1 95 201 558 275 583 (mean
8 1 2 2 93 285 567 305 578 +
8 1 2 3 91 316 557 393 583 stdev).
9 1 1 1 95 294 558 295 578 " "
10 1 1 1 80 201 657 220 682 is
10 1 1 2 87 233 666 254 673 =
10 1 1 3 75 266 656 291 682 h,
10 1 1 4 95 305 666 325 673 =
10 1 1 5 95 339 657 412 682 1725).
10 1 1 6 96 425 657 500 677 Dense
10 1 1 7 96 510 656 552 677 and
10 1 1 8 96 561 664 638 683 sparse
10 1 1 9 96 648 656 741 677 variants
10 1 1 10 96 751 656 805 677 have
10 1 2 1 96 201 694 236 715 the
10 1 2 2 96 250 701 310 715 same
10 1 2 3 96 323 694 415 715 number
10 1 2 4 96 427 694 453 715 of
10 1 2 5 96 463 698 594 722 parameters
10 1 2 6 88 608 694 642 715 for
10 1 2 7 88 668 694 680 715 N
10 1 2 8 95 698 704 719 711 =
10 1 2 9 95 737 695 750 716 3
10 1 2 10 96 764 694 806 715 and
10 1 3 1 58 200 739 216 758 7
10 1 3 2 96 231 741 251 748 =
10 1 3 3 95 267 732 340 753 0.555.
10 1 3 4 96 358 731 429 752 These
10 1 3 5 97 441 731 515 752 values
10 1 3 6 97 527 738 563 752 are
10 1 3 7 97 574 731 677 752 obtained
10 1 3 8 93 688 732 716 759 by
10 1 3 9 93 728 731 805 752 identi-
10 1 4 1 96 201 769 263 797 fying
10 1 4 2 96 274 770 327 790 both
10 1 4 3 96 339 769 484 797 expressions.
10 1 4 4 96 504 770 561 790 Note
10 1 4 5 96 573 769 617 790 that
10 1 4 6 96 629 769 664 790 the
10 1 4 7 96 676 769 771 797 equality
10 1 4 8 96 783 769 805 790 in
10 1 5 1 96 201 807 275 828 model
10 1 5 2 96 288 811 420 835 parameters
10 1 5 3 96 433 807 468 828 for
10 1 5 4 96 481 808 516 828 the
10 1 5 5 96 530 807 597 828 dense
10 1 5 6 95 611 807 654 828 and
10 1 5 7 95 668 814 741 835 sparse
10 1 5 8 96 755 814 805 828 case
10 1 6 1 96 200 844 264 865 holds
10 1 6 2 96 277 844 329 872 only
10 1 6 3 96 342 844 515 872 approximately
10 1 6 4 96 527 845 570 865 due
10 1 6 5 97 582 848 604 865 to
10 1 6 6 96 616 844 724 872 rounding
10 1 6 7 96 736 851 805 865 errors
10 1 7 1 93 201 882 223 903 in
10 1 7 2 82 234 880 292 910 (yis)
10 1 7 3 92 303 882 345 903 and
10 1 7 4 88 356 880 454 911 (hs/N).
10 2 1 1 96 231 920 308 948 Figure
10 2 1 2 71 324 921 332 941 1
10 2 1 3 95 347 920 444 948 displays
10 2 1 4 10 457 920 518 946 Wy;
10 2 1 5 91 532 921 574 941 and
10 2 1 6 17 586 920 642 946 W»,
10 2 1 7 97 656 920 691 941 for
10 2 1 8 93 702 920 737 941 the
10 2 1 9 92 750 920 805 941 mid-
10 2 2 1 93 201 958 236 979 dle
10 2 2 2 96 247 959 311 986 layer,
10 2 2 3 96 323 958 396 979 which
10 2 2 4 96 406 958 445 979 has
10 2 2 5 96 456 958 517 979 close
10 2 2 6 96 528 962 549 979 to
10 2 2 7 96 562 959 616 979 11M
10 2 2 8 96 626 962 757 986 parameters
10 2 2 9 96 768 962 806 979 out
10 2 3 1 97 201 996 227 1017 of
10 2 3 2 96 235 996 271 1017 the
10 2 3 3 96 282 997 334 1017 total
10 2 3 4 96 345 996 371 1017 of
10 2 3 5 96 380 997 435 1017 24M
10 2 3 6 96 446 996 468 1017 in
10 2 3 7 96 479 996 514 1017 the
10 2 3 8 96 525 996 598 1017 whole
10 2 3 9 95 609 996 689 1017 model.
10 2 3 10 96 707 997 727 1017 A
10 2 3 11 96 738 996 805 1017 dense
10 2 4 1 96 200 1033 275 1054 model
10 2 4 2 96 286 1033 339 1054 with
10 2 4 3 96 350 1033 431 1054 hidden
10 2 4 4 92 443 1033 488 1054 size
10 2 4 5 92 500 1033 515 1054 h
10 2 4 6 96 530 1043 550 1050 =
10 2 4 7 96 568 1034 624 1055 1725
10 2 4 8 96 636 1033 710 1054 would
10 2 4 9 96 721 1033 805 1061 require
10 2 5 1 96 201 1072 256 1093 46M
10 2 5 2 96 268 1075 406 1099 parameters,
10 2 5 3 96 419 1071 472 1092 with
10 2 5 4 96 484 1072 539 1092 24M
10 2 5 5 96 551 1071 573 1092 in
10 2 5 6 96 585 1071 620 1092 the
10 2 5 7 93 632 1071 714 1092 middle
10 2 5 8 92 726 1071 805 1092 LSTM
10 2 6 1 95 201 1109 271 1130 alone.
10 3 1 1 96 231 1147 302 1168 Given
10 3 1 2 96 318 1148 354 1168 the
10 3 1 3 91 370 1151 444 1175 strong
10 3 1 4 91 459 1148 649 1175 hyperparameter
10 3 1 5 96 664 1147 805 1175 dependence
10 3 2 1 96 201 1185 227 1206 of
10 3 2 2 92 241 1185 276 1206 the
10 3 2 3 91 293 1185 452 1206 AWD-LSTM
10 3 2 4 95 469 1185 550 1210 model,
10 3 2 5 96 569 1185 611 1206 and
10 3 2 6 96 628 1185 663 1206 the
10 3 2 7 92 680 1185 760 1206 known
10 3 2 8 92 777 1185 805 1206 is-
10 3 3 1 96 201 1229 251 1243 sues
10 3 3 2 96 268 1222 290 1243 in
10 3 3 3 96 307 1222 438 1250 objectively
10 3 3 4 96 455 1222 578 1250 evaluating
10 3 3 5 96 595 1222 703 1250 language
10 3 3 6 96 720 1222 805 1243 models
10 3 4 1 96 201 1260 278 1286 (Melis
10 3 4 2 93 292 1264 313 1281 et
10 3 4 3 90 326 1260 360 1285 al.,
10 3 4 4 96 375 1260 451 1286 2017),
10 3 4 5 96 466 1267 500 1281 we
10 3 4 6 96 514 1260 607 1281 decided
10 3 4 7 96 620 1264 642 1281 to
10 3 4 8 96 655 1260 711 1288 keep
10 3 4 9 93 725 1260 753 1281 all
10 3 4 10 93 767 1260 805 1288 hy-
10 3 5 1 92 200 1302 370 1326 perparameters
10 3 5 2 88 386 1298 438 1324 (i.e.,
10 3 5 3 96 455 1298 548 1326 dropout
10 3 5 4 96 563 1302 618 1319 rates
10 3 5 5 93 634 1298 676 1319 and
10 3 5 6 91 691 1298 805 1326 optimiza-
10 3 6 1 96 201 1335 246 1356 tion
10 3 6 2 96 257 1335 356 1361 scheme)
10 3 6 3 96 367 1342 390 1356 as
10 3 6 4 96 401 1335 423 1356 in
10 3 6 5 96 434 1335 469 1356 the
10 3 6 6 96 480 1335 668 1363 implementation
10 3 6 7 93 679 1335 737 1356 from
10 3 6 8 91 747 1336 805 1356 Mer-
10 3 7 1 96 201 1373 231 1401 ity
10 3 7 2 96 244 1377 265 1394 et
10 3 7 3 94 277 1373 303 1394 al.
10 3 7 4 93 318 1368 416 1399 (2017),
10 3 7 5 96 430 1373 543 1401 including
10 3 7 6 96 556 1373 591 1394 the
10 3 7 7 96 604 1373 685 1401 weight
10 3 7 8 96 698 1373 805 1401 dropping
10 3 8 1 93 201 1411 253 1432 with
10 3 8 2 93 265 1419 280 1438 p
10 3 8 3 93 298 1421 319 1428 =
10 3 8 4 93 338 1412 374 1433 0.5
10 3 8 5 96 388 1411 410 1432 in
10 3 8 6 96 423 1411 459 1432 the
10 3 8 7 93 472 1418 546 1439 sparse
10 3 8 8 30 559 1411 620 1437 Wp,;,
10 3 8 9 92 635 1411 743 1432 matrices.
10 3 8 10 92 767 1412 805 1432 Ta-
10 3 9 1 95 200 1448 236 1469 ble
10 3 9 2 94 255 1449 262 1469 1
10 3 9 3 94 282 1448 354 1469 shows
10 3 9 4 96 371 1448 406 1469 the
10 3 9 5 96 422 1452 463 1469 test
10 3 9 6 96 478 1448 599 1476 perplexity
10 3 9 7 96 615 1455 644 1469 on
10 3 9 8 96 660 1455 672 1469 a
10 3 9 9 96 687 1448 806 1476 processed
10 3 10 1 92 201 1486 288 1507 version
10 3 10 2 91 301 1486 412 1512 (Mikolov
10 3 10 3 93 425 1490 446 1507 et
10 3 10 4 92 458 1487 492 1511 al.,
10 3 10 5 96 506 1486 574 1512 2010)
10 3 10 6 96 587 1486 613 1507 of
10 3 10 7 96 623 1487 659 1507 the
10 3 10 8 93 671 1487 730 1507 Penn
10 3 10 9 92 742 1487 805 1507 Tree-
10 3 11 1 97 200 1523 259 1544 bank
10 3 11 2 96 270 1524 343 1549 (PTB)
10 3 11 3 96 356 1524 455 1549 (Marcus
10 3 11 4 93 467 1527 488 1544 et
10 3 11 5 93 499 1523 533 1548 al.,
10 3 11 6 96 549 1523 622 1549 1993),
10 3 11 7 96 635 1523 688 1544 both
10 3 11 8 96 699 1523 752 1544 with
10 3 11 9 96 764 1523 806 1544 and
10 3 12 1 96 201 1561 292 1582 without
10 3 12 2 93 302 1561 338 1582 the
10 3 12 3 91 351 1561 462 1582 ‘finetune’
10 3 12 4 57 476 1556 541 1589 step®,
10 3 12 5 96 553 1561 678 1589 displaying
10 3 12 6 96 688 1568 753 1582 mean
10 3 12 7 96 764 1561 806 1582 and
10 3 13 1 96 201 1600 302 1620 standard
10 3 13 2 96 316 1599 427 1620 deviation
10 3 13 3 96 441 1606 494 1620 over
10 3 13 4 96 507 1600 519 1620 5
10 3 13 5 96 536 1599 638 1620 different
10 3 13 6 88 652 1606 709 1620 runs.
10 3 13 7 88 737 1599 805 1620 With-
10 3 14 1 93 201 1640 238 1657 out
10 3 14 2 93 249 1636 377 1664 finetuning,
10 3 14 3 96 389 1636 424 1657 the
10 3 14 4 96 435 1643 509 1664 sparse
10 3 14 5 96 520 1636 594 1657 model
10 3 14 6 93 605 1636 748 1664 consistently
10 3 14 7 91 759 1643 805 1664 per-
10 3 15 1 96 200 1674 269 1695 forms
10 3 15 2 96 278 1674 360 1695 around
10 3 15 3 63 371 1675 378 1695 |
10 3 15 4 96 389 1674 510 1702 perplexity
10 3 15 5 97 517 1674 579 1702 point
10 3 15 6 96 586 1674 659 1699 better,
10 3 15 7 93 668 1674 765 1695 whereas
10 3 15 8 93 774 1674 805 1695 af-
10 3 16 1 93 201 1716 232 1733 ter
10 3 16 2 93 241 1712 369 1740 finetuning,
10 3 16 3 96 380 1712 415 1733 the
10 3 16 4 96 425 1712 518 1740 original
10 3 16 5 96 528 1712 622 1733 remains
10 3 16 6 96 633 1712 721 1740 slightly
10 3 16 7 96 731 1712 804 1737 better,
10 3 17 1 96 201 1749 305 1777 although
10 3 17 2 96 320 1749 363 1770 less
10 3 17 3 96 378 1749 521 1777 consistently
10 3 17 4 96 536 1756 561 1770 so
10 3 17 5 96 576 1756 628 1770 over
10 3 17 6 93 642 1749 744 1770 different
10 3 17 7 93 758 1756 805 1770 ran-
10 3 18 1 96 201 1787 253 1808 dom
10 3 18 2 95 267 1788 337 1808 seeds.
10 3 18 3 96 363 1788 401 1808 We
10 3 18 4 96 415 1787 523 1808 observed
10 3 18 5 96 536 1788 581 1808 that
10 3 18 6 96 594 1787 629 1808 the
10 3 18 7 96 644 1794 717 1815 sparse
10 3 18 8 96 731 1787 805 1808 model
10 3 19 1 92 201 1825 289 1846 overfits
10 3 19 2 96 297 1832 358 1846 more
10 3 19 3 96 366 1825 463 1853 strongly
10 3 19 4 96 471 1825 521 1846 than
10 3 19 5 96 529 1826 565 1846 the
10 3 19 6 96 572 1825 677 1850 baseline,
10 3 19 7 96 686 1825 805 1853 especially
10 3 20 1 97 201 1862 278 1890 during
10 3 20 2 93 289 1862 324 1883 the
10 3 20 3 92 336 1862 431 1883 finetune
10 3 20 4 96 443 1866 496 1890 step.
10 3 20 5 96 512 1863 551 1883 We
10 3 20 6 96 561 1862 704 1890 hypothesize
10 3 20 7 96 715 1862 760 1883 that
10 3 20 8 96 770 1862 805 1883 the
10 3 21 1 96 200 1900 367 1928 regularization
10 3 21 2 96 380 1900 447 1921 effect
10 3 21 3 96 460 1900 486 1921 of
10 3 21 4 96 497 1908 510 1921 a
10 3 21 5 96 522 1901 594 1927 priori
10 3 21 6 92 607 1900 701 1928 limiting
10 3 21 7 92 715 1900 805 1921 interac-
11 1 1 1 36 236 1956 284 1978 5Our
11 1 1 2 96 306 1961 461 1983 implementation
11 1 1 3 92 485 1961 559 1978 extends
11 1 1 4 91 583 1961 801 1983 https://github.
11 1 2 1 74 202 1989 591 2008 com/salesforce/awd-1stm-1m
11 2 1 1 25 236 2014 283 2036 ®The
11 2 1 2 88 296 2019 387 2036 ‘finetune’
11 2 1 3 97 401 2022 439 2041 step
11 2 1 4 93 450 2019 537 2036 indicates
11 2 1 5 92 548 2019 661 2041 hot-starting
11 2 1 6 97 672 2019 701 2036 the
11 2 1 7 96 712 2015 806 2046 Averaged
11 2 2 1 96 201 2047 301 2064 Stochastic
11 2 2 2 96 313 2047 399 2064 Gradient
11 2 2 3 96 410 2047 488 2064 Descent
11 2 2 4 96 500 2047 625 2070 optimization
11 2 2 5 96 636 2052 682 2064 once
11 2 2 6 96 693 2052 748 2067 more,
11 2 2 7 96 761 2047 806 2064 after
11 2 3 1 96 201 2080 323 2096 convergence
11 2 3 2 96 335 2074 353 2091 in
11 2 3 3 96 365 2074 394 2091 the
11 2 3 4 96 406 2074 462 2091 initial
11 2 3 5 96 474 2074 599 2097 optimization
11 2 3 6 93 611 2077 649 2097 step
11 2 3 7 93 662 2074 736 2096 (Merity
11 2 3 8 93 748 2077 765 2091 et
11 2 3 9 92 777 2074 805 2094 al.,
11 2 4 1 93 201 2102 262 2123 2017).
12 1 1 1 96 854 183 911 204 tions
12 1 1 2 96 919 183 1018 204 between
12 1 1 3 96 1026 183 1162 204 dimensions
12 1 1 4 96 1170 183 1224 204 does
12 1 1 5 95 1231 187 1269 204 not
12 1 1 6 96 1276 187 1418 211 compensate
12 1 1 7 96 1425 183 1460 204 for
12 1 2 1 96 854 222 890 242 the
12 1 2 2 96 901 221 1014 242 increased
12 1 2 3 96 1026 221 1202 249 expressiveness
12 1 2 4 96 1215 221 1241 242 of
12 1 2 5 96 1250 221 1285 242 the
12 1 2 6 96 1297 221 1371 242 model
12 1 2 7 96 1383 221 1425 242 due
12 1 2 8 96 1437 225 1459 242 to
12 1 3 1 96 854 260 890 280 the
12 1 3 2 97 903 259 972 287 larger
12 1 3 3 96 984 259 1066 280 hidden
12 1 3 4 96 1079 263 1133 280 state
12 1 3 5 95 1146 259 1198 280 size.
12 1 3 6 93 1222 260 1311 280 Further
12 1 3 7 92 1323 259 1458 287 experimen-
12 1 4 1 97 854 296 928 321 tation,
12 1 4 2 96 943 296 996 317 with
12 1 4 3 93 1008 296 1074 317 tuned
12 1 4 4 92 1087 296 1294 324 hyperparameters,
12 1 4 5 96 1308 296 1327 317 is
12 1 4 6 96 1340 296 1424 317 needed
12 1 4 7 96 1437 300 1459 317 to
12 1 5 1 96 854 334 974 355 determine
12 1 5 2 97 982 334 1018 355 the
12 1 5 3 96 1027 334 1097 355 actual
12 1 5 4 96 1106 334 1198 355 benefits
12 1 5 5 97 1207 334 1233 355 of
12 1 5 6 92 1239 334 1366 362 predefined
12 1 5 7 92 1375 341 1458 362 sparse-
12 1 6 1 96 854 379 911 397 ness,
12 1 6 2 96 924 372 946 393 in
12 1 6 3 96 957 376 1022 393 terms
12 1 6 4 96 1033 372 1059 393 of
12 1 6 5 96 1068 373 1142 393 model
12 1 6 6 96 1153 372 1205 397 size,
12 1 6 7 96 1217 372 1322 400 resulting
12 1 6 8 96 1332 372 1458 400 perplexity,
12 1 7 1 96 855 409 897 430 and
12 1 7 2 96 906 409 1028 437 sensitivity
12 1 7 3 96 1037 413 1059 430 to
12 1 7 4 96 1068 410 1104 430 the
12 1 7 5 96 1113 409 1190 430 choice
12 1 7 6 93 1199 409 1225 430 of
12 1 7 7 92 1232 409 1439 437 hyperparameters.
13 1 1 1 95 854 446 869 469 4
13 1 1 2 95 904 446 998 476 Sparse
13 1 1 3 96 1008 446 1087 470 Word
13 1 1 4 96 1097 446 1273 476 Embeddings
14 1 1 1 95 885 505 956 526 Given
14 1 1 2 95 968 512 980 526 a
14 1 1 3 95 991 505 1122 533 vocabulary
14 1 1 4 92 1134 505 1186 526 with
14 1 1 5 92 1199 505 1220 527 V
14 1 1 6 96 1232 506 1310 530 words,
14 1 1 7 96 1323 512 1357 526 we
14 1 1 8 96 1369 509 1426 526 want
14 1 1 9 96 1437 509 1459 526 to
14 1 2 1 96 854 547 964 564 construct
14 1 2 2 96 979 547 1054 564 vector
14 1 2 3 96 1068 543 1250 571 representations
14 1 2 4 96 1266 543 1292 564 of
14 1 2 5 93 1305 543 1379 571 length
14 1 2 6 83 1395 543 1409 564 k
14 1 2 7 97 1425 543 1460 564 for
14 1 3 1 96 854 581 908 601 each
14 1 3 2 96 920 581 980 601 word
14 1 3 3 96 992 580 1046 601 such
14 1 3 4 96 1057 581 1102 601 that
14 1 3 5 96 1113 580 1148 601 the
14 1 3 6 96 1160 580 1212 601 total
14 1 3 7 96 1223 580 1315 601 number
14 1 3 8 93 1326 580 1352 601 of
14 1 3 9 92 1361 587 1459 608 parame-
14 1 4 1 96 854 622 896 639 ters
14 1 4 2 93 906 618 990 639 needed
14 1 4 3 89 1000 618 1051 644 (i.e.,
14 1 4 4 96 1062 625 1168 639 non-zero
14 1 4 5 96 1177 618 1273 644 entries),
14 1 4 6 96 1284 618 1302 639 is
14 1 4 7 96 1312 618 1400 639 smaller
14 1 4 8 97 1408 618 1459 639 than
14 1 5 1 89 855 655 905 677 kV.
14 1 5 2 95 927 656 966 676 We
14 1 5 3 96 978 655 1091 676 introduce
14 1 5 4 96 1104 662 1146 676 one
14 1 5 5 96 1159 662 1207 683 way
14 1 5 6 96 1220 659 1242 676 to
14 1 5 7 95 1254 655 1283 676 do
14 1 5 8 96 1296 655 1337 676 this
14 1 5 9 96 1350 655 1418 676 based
14 1 5 10 97 1430 662 1459 676 on
14 1 6 1 96 854 693 915 714 word
14 1 6 2 96 927 693 1064 721 frequencies
14 1 6 3 93 1077 693 1176 719 (Section
14 1 6 4 92 1188 694 1203 719 ),
14 1 6 5 96 1217 693 1259 714 and
14 1 6 6 93 1270 697 1357 721 present
14 1 6 7 92 1368 693 1459 721 part-of-
14 1 7 1 96 855 732 935 759 speech
14 1 7 2 96 944 731 1034 759 tagging
14 1 7 3 96 1043 731 1188 759 experiments
14 1 7 4 93 1199 731 1297 757 (Section
14 1 7 5 93 1307 732 1321 757 ).
14 2 1 1 46 854 768 890 789 4.1
14 2 1 2 45 923 768 1142 795 Word-Frequency
14 2 1 3 96 1151 768 1223 790 based
14 2 1 4 96 1233 768 1381 795 Embedding
14 2 1 5 96 1390 768 1441 790 Size
15 1 1 1 93 884 817 1013 838 Predefined
15 1 1 2 92 1036 824 1162 845 sparseness
15 1 1 3 96 1185 817 1207 838 in
15 1 1 4 96 1231 817 1291 838 word
15 1 1 5 96 1314 817 1459 845 embeddings
15 1 2 1 96 855 859 955 876 amounts
15 1 2 2 96 967 859 989 876 to
15 1 2 3 96 1000 855 1103 883 deciding
15 1 2 4 96 1115 855 1187 876 which
15 1 2 5 96 1198 855 1306 883 positions
15 1 2 6 97 1318 855 1340 876 in
15 1 2 7 96 1352 855 1387 876 the
15 1 2 8 96 1399 855 1459 876 word
15 1 3 1 96 854 892 987 920 embedding
15 1 3 2 93 998 892 1076 913 matrix
15 1 3 3 90 1098 892 1108 913 E
15 1 3 4 71 1124 897 1139 914 ¢
15 1 3 5 22 1155 887 1225 913 RY
15 1 3 6 22 1196 883 1221 925 **
15 1 3 7 96 1239 892 1318 913 should
15 1 3 8 96 1328 893 1356 913 be
15 1 3 9 96 1367 892 1426 913 fixed
15 1 3 10 96 1437 896 1459 913 to
15 1 4 1 96 854 937 911 955 zero,
15 1 4 2 96 925 930 983 958 prior
15 1 4 3 97 995 934 1016 951 to
15 1 4 4 96 1029 930 1127 958 training.
15 1 4 5 96 1148 931 1186 951 We
15 1 4 6 96 1198 930 1271 951 define
15 1 4 7 97 1283 931 1318 951 the
15 1 4 8 96 1331 930 1423 951 fraction
15 1 4 9 97 1435 930 1461 951 of
15 1 5 1 97 854 967 959 988 trainable
15 1 5 2 96 966 967 1045 988 entries
15 1 5 3 93 1053 967 1075 988 in
15 1 5 4 92 1083 967 1104 988 E
15 1 5 5 97 1112 974 1135 988 as
15 1 5 6 96 1143 968 1178 988 the
15 1 5 7 96 1185 967 1319 995 embedding
15 1 5 8 96 1326 967 1412 995 density
15 1 5 9 64 1420 966 1458 993 dz.
15 1 6 1 96 854 1006 892 1026 We
15 1 6 2 96 900 1005 1043 1033 hypothesize
15 1 6 3 96 1052 1005 1096 1026 that
15 1 6 4 96 1103 1012 1149 1026 rare
15 1 6 5 96 1157 1005 1229 1026 words
15 1 6 6 96 1237 1012 1278 1026 can
15 1 6 7 96 1285 1005 1313 1026 be
15 1 6 8 96 1320 1005 1459 1033 represented
15 1 7 1 97 855 1043 907 1064 with
15 1 7 2 96 917 1043 985 1064 fewer
15 1 7 3 96 994 1047 1125 1071 parameters
15 1 7 4 96 1136 1044 1187 1064 than
15 1 7 5 96 1198 1043 1297 1071 frequent
15 1 7 6 96 1308 1043 1386 1068 words,
15 1 7 7 96 1399 1043 1459 1064 since
15 1 8 1 96 854 1080 904 1108 they
15 1 8 2 96 917 1080 969 1108 only
15 1 8 3 95 983 1087 1062 1108 appear
15 1 8 4 95 1075 1080 1096 1101 in
15 1 8 5 96 1109 1087 1161 1108 very
15 1 8 6 96 1175 1080 1265 1108 specific
15 1 8 7 96 1278 1084 1383 1101 contexts.
15 1 8 8 96 1406 1080 1459 1101 This
15 1 9 1 96 855 1118 900 1139 will
15 1 9 2 96 915 1118 942 1139 be
15 1 9 3 96 958 1118 1101 1146 investigated
15 1 9 4 96 1116 1118 1296 1146 experimentally
15 1 9 5 96 1311 1118 1333 1139 in
15 1 9 6 93 1349 1118 1438 1139 Section
15 1 9 7 92 1454 1136 1458 1139 .
15 1 10 1 95 854 1157 919 1177 Word
15 1 10 2 96 931 1163 1063 1177 occurrence
15 1 10 3 96 1075 1156 1212 1184 frequencies
15 1 10 4 96 1225 1156 1279 1177 have
15 1 10 5 96 1292 1163 1304 1177 a
15 1 10 6 93 1315 1156 1396 1184 typical
15 1 10 7 92 1408 1156 1459 1184 Zip-
15 1 11 1 92 855 1193 898 1214 fian
15 1 11 2 96 907 1197 981 1214 nature
15 1 11 3 96 991 1193 1108 1221 (Manning
15 1 11 4 93 1117 1197 1138 1214 et
15 1 11 5 92 1147 1193 1182 1218 al.,
15 1 11 6 96 1192 1193 1267 1219 2008),
15 1 11 7 96 1278 1193 1330 1214 with
15 1 11 8 96 1339 1200 1404 1221 many
15 1 11 9 96 1413 1200 1459 1214 rare
15 1 12 1 96 855 1231 897 1252 and
15 1 12 2 95 905 1231 948 1252 few
15 1 12 3 96 957 1231 1033 1259 highly
15 1 12 4 96 1043 1231 1142 1259 frequent
15 1 12 5 96 1151 1235 1223 1252 terms.
15 1 12 6 96 1235 1231 1301 1256 Thus,
15 1 12 7 96 1310 1231 1459 1259 representing
15 1 13 1 97 854 1270 890 1290 the
15 1 13 2 96 901 1269 953 1297 long
15 1 13 3 96 964 1269 1001 1290 tail
15 1 13 4 96 1012 1269 1038 1290 of
15 1 13 5 96 1047 1276 1093 1290 rare
15 1 13 6 96 1104 1273 1169 1290 terms
15 1 13 7 96 1181 1269 1233 1290 with
15 1 13 8 96 1245 1269 1303 1290 short
15 1 13 9 96 1314 1269 1459 1297 embeddings
15 1 14 1 96 855 1306 934 1327 should
15 1 14 2 96 943 1306 1025 1334 greatly
15 1 14 3 96 1034 1306 1113 1327 reduce
15 1 14 4 96 1123 1313 1222 1334 memory
15 1 14 5 96 1231 1306 1394 1334 requirements.
15 2 1 1 96 885 1345 908 1365 In
15 2 1 2 96 918 1344 954 1365 the
15 2 1 3 96 964 1351 1014 1365 case
15 2 1 4 96 1025 1344 1051 1365 of
15 2 1 5 96 1059 1351 1071 1365 a
15 2 1 6 96 1081 1344 1123 1365 low
15 2 1 7 96 1134 1344 1220 1365 desired
15 2 1 8 96 1230 1344 1363 1372 embedding
15 2 1 9 96 1373 1344 1459 1372 density
15 2 2 1 26 855 1380 893 1407 dz,
15 2 2 2 90 903 1388 936 1402 we
15 2 2 3 96 946 1385 1003 1402 want
15 2 2 4 96 1012 1385 1034 1402 to
15 2 2 5 95 1043 1388 1094 1402 save
15 2 2 6 96 1103 1388 1132 1402 on
15 2 2 7 96 1141 1381 1176 1402 the
15 2 2 8 96 1185 1388 1231 1402 rare
15 2 2 9 96 1240 1381 1319 1406 words,
15 2 2 10 96 1329 1381 1351 1402 in
15 2 2 11 96 1360 1385 1426 1402 terms
15 2 2 12 96 1435 1381 1461 1402 of
15 2 3 1 96 855 1419 967 1447 assigning
15 2 3 2 96 980 1419 1084 1440 trainable
15 2 3 3 96 1097 1423 1235 1447 parameters,
15 2 3 4 96 1250 1420 1292 1440 and
15 2 3 5 96 1305 1419 1369 1440 focus
15 2 3 6 96 1382 1426 1411 1440 on
15 2 3 7 96 1424 1419 1459 1440 the
15 2 4 1 96 854 1457 922 1478 fewer
15 2 4 2 96 931 1464 991 1478 more
15 2 4 3 96 1001 1458 1093 1485 popular
15 2 4 4 96 1102 1457 1181 1478 words.
15 2 4 5 96 1195 1458 1231 1478 An
15 2 4 6 96 1240 1457 1380 1485 exponential
15 2 4 7 96 1390 1457 1459 1485 decay
15 2 5 1 96 854 1494 876 1515 in
15 2 5 2 96 889 1495 924 1515 the
15 2 5 3 96 937 1494 1029 1515 number
15 2 5 4 96 1041 1494 1067 1515 of
15 2 5 5 96 1078 1494 1149 1515 words
15 2 5 6 96 1163 1494 1207 1515 that
15 2 5 7 96 1220 1501 1255 1515 are
15 2 5 8 96 1268 1494 1371 1522 assigned
15 2 5 9 96 1383 1494 1460 1522 longer
15 2 6 1 96 854 1532 1036 1560 representations
15 2 6 2 97 1047 1532 1065 1553 is
15 2 6 3 97 1077 1539 1119 1553 one
15 2 6 4 96 1130 1532 1227 1560 possible
15 2 6 5 96 1238 1539 1287 1560 way
15 2 6 6 96 1298 1536 1320 1553 to
15 2 6 7 96 1331 1532 1459 1560 implement
15 2 7 1 96 854 1570 903 1591 this.
15 2 7 2 95 916 1571 939 1591 In
15 2 7 3 96 949 1571 1011 1591 other
15 2 7 4 96 1020 1570 1098 1595 words,
15 2 7 5 96 1109 1577 1143 1591 we
15 2 7 6 96 1152 1577 1246 1598 propose
15 2 7 7 96 1256 1574 1278 1591 to
15 2 7 8 96 1287 1570 1342 1591 have
15 2 7 9 93 1352 1570 1387 1591 the
15 2 7 10 92 1396 1577 1459 1591 num-
15 2 8 1 96 854 1607 892 1628 ber
15 2 8 2 96 902 1607 928 1628 of
15 2 8 3 96 937 1608 1009 1628 words
15 2 8 4 96 1020 1607 1064 1628 that
15 2 8 5 96 1074 1607 1159 1628 receive
15 2 8 6 94 1170 1614 1183 1628 a
15 2 8 7 94 1193 1607 1297 1628 trainable
15 2 8 8 95 1308 1611 1429 1635 parameter
15 2 8 9 96 1439 1611 1459 1628 at
15 2 9 1 93 854 1645 979 1666 dimension
15 2 9 2 91 986 1646 999 1672 j
15 2 9 3 96 1009 1645 1111 1666 decrease
15 2 9 4 96 1120 1645 1172 1666 with
15 2 9 5 96 1181 1652 1193 1666 a
15 2 9 6 91 1201 1645 1271 1666 factor
15 2 9 7 57 1279 1640 1306 1666 a/
15 2 9 8 89 1318 1646 1344 1671 (a
15 2 9 9 89 1356 1650 1372 1667 €
15 2 9 10 64 1382 1643 1411 1674 ]0,
15 2 9 11 66 1421 1643 1458 1674 1).
15 2 10 1 96 854 1684 895 1704 For
15 2 10 2 96 902 1690 915 1704 a
15 2 10 3 96 922 1683 986 1711 given
15 2 10 4 96 994 1683 1087 1704 fraction
15 2 10 5 38 1095 1682 1133 1708 6,
15 2 10 6 96 1142 1684 1178 1704 the
15 2 10 7 95 1185 1687 1306 1711 parameter
15 2 10 8 94 1313 1691 1330 1704 a
15 2 10 9 94 1338 1690 1379 1704 can
15 2 10 10 93 1387 1683 1414 1704 be
15 2 10 11 92 1422 1683 1458 1704 de-
15 2 11 1 95 854 1720 961 1741 termined
15 2 11 2 96 971 1720 1029 1741 from
15 2 11 3 96 1039 1720 1149 1748 requiring
15 2 11 4 97 1160 1720 1195 1741 the
15 2 11 5 97 1206 1720 1258 1741 total
15 2 11 6 96 1269 1720 1361 1741 number
15 2 11 7 93 1371 1720 1397 1741 of
15 2 11 8 92 1405 1727 1459 1741 non-
15 2 12 1 96 854 1765 905 1779 zero
15 2 12 2 96 916 1758 1049 1786 embedding
15 2 12 3 96 1060 1762 1192 1786 parameters
15 2 12 4 96 1204 1762 1225 1779 to
15 2 12 5 96 1237 1762 1327 1779 amount
15 2 12 6 96 1338 1762 1360 1779 to
15 2 12 7 96 1371 1765 1384 1779 a
15 2 12 8 96 1394 1758 1459 1786 given
15 2 13 1 96 854 1795 947 1816 fraction
15 2 13 2 29 956 1794 969 1816 6
15 2 13 3 96 997 1795 1023 1816 of
15 2 13 4 96 1030 1795 1058 1816 all
15 2 13 5 97 1067 1799 1206 1823 parameters:
16 1 1 1 49 1193 1928 1233 1951 LZ
17 1 1 1 96 885 2023 927 2044 and
17 1 1 2 96 936 2023 1079 2051 numerically
17 1 1 3 96 1089 2023 1176 2051 solving
17 1 1 4 96 1185 2023 1220 2044 for
17 1 1 5 93 1228 2031 1252 2044 a.
17 1 2 1 96 884 2060 962 2088 Figure
17 1 2 2 96 971 2060 984 2081 2
17 1 2 3 96 993 2060 1054 2088 gives
17 1 2 4 96 1063 2061 1175 2088 examples
17 1 2 5 96 1184 2060 1210 2081 of
17 1 2 6 96 1216 2060 1349 2088 embedding
17 1 2 7 96 1358 2060 1459 2081 matrices
17 1 3 1 96 854 2098 907 2119 with
17 1 3 2 96 919 2098 1009 2126 varying
17 1 3 3 49 1022 2097 1059 2124 dg.
17 1 3 4 96 1081 2099 1122 2119 For
17 1 3 5 96 1133 2105 1145 2119 a
17 1 3 6 96 1157 2098 1289 2126 vocabulary
17 1 3 7 96 1301 2098 1327 2119 of
17 1 3 8 96 1337 2098 1381 2119 44k
17 1 3 9 96 1393 2102 1459 2119 terms
