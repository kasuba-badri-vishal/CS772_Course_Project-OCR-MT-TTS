block para line word confidence X1 Y1 X2 Y2 token
1 1 1 1 96 254 428 295 453 Fig.
1 1 1 2 96 308 428 326 447 3:
1 1 1 3 96 338 428 451 447 Controller
1 1 1 4 96 462 428 527 447 Block
1 1 1 5 96 538 428 633 453 Diagram
2 1 1 1 96 166 529 198 548 IV.
2 1 1 2 95 216 530 421 548 REINFORCEMENT
2 1 1 3 95 430 530 555 548 LEARNING
2 1 1 4 96 564 530 683 548 METHODS
2 1 1 5 95 693 533 721 548 AS
2 1 2 1 96 356 563 530 582 CONTROLLERS
3 1 1 1 96 132 609 153 627 In
3 1 1 2 95 171 608 206 631 [1],
3 1 1 3 96 224 614 254 627 we
3 1 1 4 95 270 608 351 627 worked
3 1 1 5 95 367 614 393 627 on
3 1 1 6 96 410 608 521 627 traditional
3 1 1 7 96 538 608 660 627 Controllers
3 1 1 8 96 677 608 716 627 like
3 1 1 9 96 733 608 782 631 PID,
3 1 2 1 93 104 642 170 666 Fuzzy
3 1 2 2 91 183 642 284 664 PD,PD+I
3 1 2 3 92 298 641 317 661 &
3 1 2 4 88 330 641 385 665 LQR
3 1 2 5 92 398 657 401 660 .
3 1 2 6 96 415 641 457 660 The
3 1 2 7 96 470 641 549 666 biggest
3 1 2 8 96 561 641 652 666 problem
3 1 2 9 96 665 641 713 660 with
3 1 2 10 96 726 641 783 660 those
3 1 3 1 96 104 674 196 693 methods
3 1 3 2 96 205 674 221 693 is
3 1 3 3 93 229 674 270 693 that
3 1 3 4 92 278 690 282 697 ,
3 1 3 5 96 291 674 336 699 they
3 1 3 6 96 345 674 395 693 need
3 1 3 7 96 403 677 423 693 to
3 1 3 8 97 431 674 456 693 be
3 1 3 9 96 464 674 524 693 tuned
3 1 3 10 96 532 674 637 699 manually.
3 1 3 11 96 647 674 680 697 So,
3 1 3 12 96 689 674 782 699 reaching
3 1 4 1 96 104 707 187 732 optimal
3 1 4 2 96 202 707 270 726 values
3 1 4 3 95 284 707 308 726 of
3 1 4 4 96 320 707 443 726 Controllers
3 1 4 5 96 458 707 547 732 depends
3 1 4 6 96 561 713 588 726 on
3 1 4 7 96 602 713 661 732 many
3 1 4 8 96 676 707 729 726 trials
3 1 4 9 96 745 707 783 726 and
3 1 5 1 96 104 747 173 760 errors.
3 1 5 2 96 186 742 248 766 Many
3 1 5 3 96 262 747 273 760 a
3 1 5 4 96 284 741 343 760 times
3 1 5 5 96 355 741 454 766 optimum
3 1 5 6 93 466 741 535 760 values
3 1 5 7 91 547 741 610 760 aren’t
3 1 5 8 96 622 741 707 760 reached
3 1 5 9 96 719 744 738 760 at
3 1 5 10 96 750 741 781 760 all.
3 1 6 1 96 104 774 146 793 The
3 1 6 2 96 161 774 240 799 biggest
3 1 6 3 96 255 774 330 793 benefit
3 1 6 4 96 345 774 369 793 of
3 1 6 5 96 382 774 546 793 Reinforcement
3 1 6 6 96 561 774 650 799 learning
3 1 6 7 95 666 774 782 799 algorithms
3 1 7 1 95 105 813 125 826 as
3 1 7 2 95 139 807 262 826 Controllers
3 1 7 3 96 276 807 293 826 is
3 1 7 4 92 307 807 347 826 that
3 1 7 5 89 361 823 365 830 ,
3 1 7 6 96 379 807 411 826 the
3 1 7 7 96 425 807 493 826 model
3 1 7 8 96 507 810 563 826 tunes
3 1 7 9 95 577 807 633 826 itself
3 1 7 10 96 645 811 665 826 to
3 1 7 11 96 678 807 737 826 reach
3 1 7 12 96 751 807 783 826 the
3 1 8 1 96 104 840 209 865 Optimum
3 1 8 2 96 227 840 301 859 values.
3 1 8 3 96 320 840 362 859 The
3 1 8 4 96 380 840 485 865 following
3 1 8 5 96 504 844 544 859 two
3 1 8 6 95 562 840 649 859 sections
3 1 8 7 93 667 840 746 859 discuss
3 1 8 8 93 764 840 783 864 Q
3 1 9 1 96 104 873 202 898 Learning
3 1 9 2 96 214 873 253 892 and
3 1 9 3 91 263 874 320 898 Deep
3 1 9 4 91 332 873 350 897 Q
3 1 9 5 96 361 873 462 892 Network.
3 2 1 1 88 102 908 124 926 A.
3 2 1 2 88 143 907 160 931 Q
3 2 1 3 96 171 908 272 932 Learning
4 1 1 1 91 132 951 159 975 Q-
4 1 1 2 96 172 951 262 976 learning
4 1 1 3 96 274 957 315 970 was
4 1 1 4 95 328 951 440 976 developed
4 1 1 5 96 452 951 478 976 by
4 1 1 6 95 491 951 621 976 Christopher
4 1 1 7 96 633 951 684 970 John
4 1 1 8 96 696 951 783 970 Cornish
4 1 2 1 92 104 984 190 1009 Hellaby
4 1 2 2 96 207 984 295 1003 Watkins
4 1 2 3 96 314 985 348 1007 [7].
4 1 2 4 96 366 984 481 1009 According
4 1 2 5 96 498 987 518 1003 to
4 1 2 6 93 535 984 629 1007 Watkins,
4 1 2 7 14 647 984 673 1003 ”it
4 1 2 8 96 689 984 782 1009 provides
4 1 3 1 96 105 1020 173 1042 agents
4 1 3 2 96 189 1017 236 1036 with
4 1 3 3 96 251 1017 283 1036 the
4 1 3 4 96 298 1017 405 1042 capability
4 1 3 5 96 421 1017 445 1036 of
4 1 3 6 95 458 1017 547 1042 learning
4 1 3 7 95 562 1020 582 1036 to
4 1 3 8 96 597 1021 629 1036 act
4 1 3 9 96 643 1017 747 1042 optimally
4 1 3 10 96 763 1017 783 1036 in
4 1 4 1 96 104 1050 223 1069 Markovian
4 1 4 2 96 239 1050 331 1069 domains
4 1 4 3 96 347 1050 373 1075 by
4 1 4 4 96 389 1050 531 1075 experiencing
4 1 4 5 96 547 1050 579 1069 the
4 1 4 6 96 594 1056 745 1075 consequences
4 1 4 7 96 761 1050 785 1069 of
4 1 5 1 96 104 1084 187 1107 actions,
4 1 5 2 96 204 1084 287 1103 without
4 1 5 3 96 301 1084 401 1109 requiring
4 1 5 4 96 417 1084 471 1103 them
4 1 5 5 96 486 1088 506 1103 to
4 1 5 6 96 520 1084 576 1103 build
4 1 5 7 96 591 1090 647 1109 maps
4 1 5 8 96 663 1084 687 1103 of
4 1 5 9 93 700 1084 732 1103 the
4 1 5 10 92 747 1084 782 1103 do-
4 1 6 1 96 104 1117 186 1136 mains.”
4 1 6 2 96 199 1117 233 1140 [8].
4 1 6 3 91 246 1117 267 1136 In
4 1 6 4 91 284 1123 289 1136 a
4 1 6 5 96 300 1117 419 1136 Markovian
4 1 6 6 93 430 1117 518 1140 domain,
4 1 6 7 91 530 1116 549 1141 Q
4 1 6 8 92 561 1117 660 1136 function-
4 1 6 9 96 672 1117 704 1136 the
4 1 6 10 96 715 1117 783 1136 model
4 1 7 1 96 104 1153 124 1169 to
4 1 7 2 97 134 1150 159 1169 be
4 1 7 3 96 170 1150 277 1175 generated
4 1 7 4 96 287 1150 345 1175 using
4 1 7 5 93 356 1150 388 1169 the
4 1 7 6 92 399 1150 514 1175 algorithm-
4 1 7 7 96 525 1150 632 1169 calculates
4 1 7 8 96 643 1150 675 1169 the
4 1 7 9 96 686 1150 783 1175 expected
4 1 8 1 96 104 1183 168 1208 utility
4 1 8 2 96 184 1183 216 1202 for
4 1 8 3 96 231 1189 242 1202 a
4 1 8 4 96 257 1183 316 1208 given
4 1 8 5 96 332 1183 387 1202 finite
4 1 8 6 93 403 1186 451 1202 state
4 1 8 7 92 468 1190 478 1202 s
4 1 8 8 96 494 1183 533 1202 and
4 1 8 9 96 548 1189 606 1208 every
4 1 8 10 96 622 1183 712 1208 possible
4 1 8 11 96 728 1183 783 1202 finite
4 1 9 1 96 105 1217 170 1236 action
4 1 9 2 96 181 1224 200 1236 a.
4 1 9 3 96 211 1217 253 1236 The
4 1 9 4 93 264 1221 322 1242 agent
4 1 9 5 93 332 1229 340 1231 -
4 1 9 6 96 351 1217 417 1236 which
4 1 9 7 96 427 1217 444 1236 is
4 1 9 8 96 455 1217 487 1236 the
4 1 9 9 97 497 1217 555 1236 robot
4 1 9 10 96 565 1217 585 1236 in
4 1 9 11 93 596 1217 634 1236 this
4 1 9 12 93 644 1223 700 1236 case-
4 1 9 13 96 711 1217 782 1236 selects
4 1 10 1 96 104 1250 136 1269 the
4 1 10 2 96 148 1250 247 1275 optimum
4 1 10 3 96 259 1250 325 1269 action
4 1 10 4 96 337 1257 350 1269 a
4 1 10 5 96 361 1250 435 1275 having
4 1 10 6 96 447 1250 479 1269 the
4 1 10 7 96 490 1250 569 1275 highest
4 1 10 8 96 581 1250 638 1269 value
4 1 10 9 93 650 1250 674 1269 of
4 1 10 10 86 684 1248 764 1276 Q(s,a)
4 1 10 11 91 778 1266 782 1273 ,
4 1 11 1 96 104 1283 142 1302 this
4 1 11 2 96 154 1283 220 1302 action
4 1 11 3 96 230 1283 329 1308 choosing
4 1 11 4 96 339 1283 381 1302 rule
4 1 11 5 96 392 1283 408 1302 is
4 1 11 6 96 420 1283 463 1302 also
4 1 11 7 96 473 1283 538 1302 called
4 1 11 8 96 549 1283 622 1308 Policy.
4 1 11 9 93 635 1283 663 1306 [8]
4 1 11 10 92 677 1299 679 1302 .
4 1 11 11 96 691 1283 782 1308 Initially,
4 1 12 1 93 104 1316 136 1335 the
4 1 12 2 87 151 1315 230 1342 Q(s,a)
4 1 12 3 96 246 1316 337 1335 function
4 1 12 4 96 351 1316 419 1335 values
4 1 12 5 96 433 1322 465 1335 are
4 1 12 6 95 479 1316 573 1335 assumed
4 1 12 7 95 587 1319 607 1335 to
4 1 12 8 96 620 1316 645 1335 be
4 1 12 9 96 659 1322 711 1335 zero.
4 1 12 10 97 726 1316 783 1335 After
4 1 13 1 96 104 1355 162 1374 every
4 1 13 2 96 174 1349 258 1374 training
4 1 13 3 91 270 1353 312 1374 step
4 1 13 4 91 324 1365 328 1372 ,
4 1 13 5 96 339 1349 371 1368 the
4 1 13 6 96 383 1349 450 1368 values
4 1 13 7 96 461 1355 493 1368 are
4 1 13 8 96 504 1349 591 1374 updated
4 1 13 9 95 601 1349 709 1374 according
4 1 13 10 96 720 1352 740 1368 to
4 1 13 11 96 751 1349 783 1368 the
4 1 14 1 96 104 1383 210 1408 following
4 1 14 2 96 221 1383 315 1408 equation
5 1 1 1 15 752 1472 782 1495 re)
6 1 1 1 96 131 1558 173 1577 The
6 1 1 2 96 189 1558 288 1583 objective
6 1 1 3 96 304 1558 335 1577 for
6 1 1 4 96 351 1558 383 1577 the
6 1 1 5 96 399 1558 469 1577 Model
6 1 1 6 96 485 1558 505 1577 in
6 1 1 7 96 521 1564 557 1577 our
6 1 1 8 96 572 1558 648 1583 project
6 1 1 9 96 664 1558 680 1577 is
6 1 1 10 96 696 1561 716 1577 to
6 1 1 11 96 732 1558 783 1583 keep
6 1 2 1 96 104 1592 119 1611 it
6 1 2 2 96 134 1592 203 1611 within
6 1 2 3 95 219 1592 281 1611 limits
6 1 2 4 90 297 1592 328 1611 ie.
6 1 2 5 91 346 1588 390 1612 +5°
6 1 2 6 86 409 1608 412 1611 .
6 1 2 7 96 429 1593 456 1611 At
6 1 2 8 92 472 1592 513 1611 first
6 1 2 9 90 530 1608 534 1614 ,
6 1 2 10 96 550 1592 583 1611 the
6 1 2 11 96 599 1592 656 1611 robot
6 1 2 12 96 672 1592 745 1615 model,
6 1 2 13 91 763 1591 782 1616 Q
6 1 3 1 96 104 1625 181 1648 matrix,
6 1 3 2 96 199 1625 268 1650 Policy
6 1 3 3 47 286 1632 301 1644 a
6 1 3 4 94 320 1631 351 1644 are
6 1 3 5 93 369 1625 479 1644 initialized
6 1 3 6 92 497 1641 501 1644 .
6 1 3 7 96 519 1625 582 1644 There
6 1 3 8 96 600 1631 632 1644 are
6 1 3 9 91 651 1631 707 1644 some
6 1 3 10 91 725 1625 782 1644 inter-
6 1 4 1 96 104 1658 169 1683 esting
6 1 4 2 96 186 1658 252 1683 points
6 1 4 3 96 270 1661 290 1677 to
6 1 4 4 96 308 1658 372 1677 make.
6 1 4 5 96 390 1658 432 1677 The
6 1 4 6 95 450 1661 509 1677 states
6 1 4 7 95 527 1664 559 1677 are
6 1 4 8 96 577 1661 612 1677 not
6 1 4 9 95 629 1658 690 1677 finite.
6 1 4 10 96 709 1658 783 1677 Within
6 1 5 1 96 104 1691 156 1710 limit
6 1 5 2 93 170 1697 230 1716 range
6 1 5 3 92 246 1707 250 1714 ,
6 1 5 4 96 266 1691 366 1710 hundreds
6 1 5 5 95 382 1691 420 1710 and
6 1 5 6 95 435 1691 544 1710 thousands
6 1 5 7 97 560 1691 584 1710 of
6 1 5 8 95 596 1691 651 1716 pitch
6 1 5 9 96 666 1691 735 1716 angles
6 1 5 10 96 751 1697 783 1710 are
6 1 6 1 96 104 1725 199 1750 possible.
6 1 6 2 96 219 1725 299 1750 Having
6 1 6 3 96 318 1725 427 1744 thousands
6 1 6 4 96 446 1725 469 1744 of
6 1 6 5 95 486 1725 578 1744 columns
6 1 6 6 95 598 1725 615 1744 is
6 1 6 7 95 634 1728 668 1744 not
6 1 6 8 95 686 1725 781 1750 possible.
6 1 7 1 96 105 1758 138 1781 So,
6 1 7 2 96 155 1758 187 1777 the
6 1 7 3 96 204 1761 253 1777 state
6 1 7 4 95 270 1758 338 1777 values
6 1 7 5 96 354 1764 406 1777 were
6 1 7 6 96 423 1758 547 1777 discretized.
6 1 7 7 96 565 1759 599 1777 We
6 1 7 8 95 615 1758 734 1777 discretized
6 1 7 9 96 751 1758 783 1777 the
6 1 8 1 96 105 1791 172 1810 values
6 1 8 2 96 193 1794 213 1810 to
6 1 8 3 96 233 1791 260 1810 20
6 1 8 4 96 280 1791 364 1810 discrete
6 1 8 5 96 385 1794 434 1810 state
6 1 8 6 96 454 1791 523 1816 angles
6 1 8 7 93 544 1791 596 1810 from
6 1 8 8 88 618 1787 675 1811 —10°
6 1 8 9 96 697 1795 717 1810 to
6 1 8 10 93 739 1787 781 1811 10°.
6 1 9 1 96 104 1825 142 1843 For
6 1 9 2 96 158 1824 224 1843 action
6 1 9 3 96 241 1824 304 1847 value,
6 1 9 4 96 322 1830 352 1843 we
6 1 9 5 96 369 1824 430 1843 chose
6 1 9 6 95 450 1824 474 1843 10
6 1 9 7 95 490 1824 584 1843 different
6 1 9 8 96 600 1824 710 1843 velocities.
6 1 9 9 96 727 1824 782 1849 They
6 1 10 1 93 105 1863 136 1876 are
6 1 10 2 62 147 1855 220 1883 [—200,
6 1 10 3 74 230 1858 295 1881 —100,
6 1 10 4 84 305 1858 357 1881 —50,
6 1 10 5 80 366 1858 418 1877 —25,
6 1 10 6 85 428 1858 480 1881 —10,
6 1 10 7 96 489 1858 520 1881 10,
6 1 10 8 94 528 1858 560 1881 25,
6 1 10 9 94 568 1858 600 1881 50,
6 1 10 10 92 609 1858 654 1881 100,
6 1 10 11 0 662 1853 782 1883 200)ms7?.
6 1 11 1 93 104 1891 146 1910 The
6 1 11 2 91 159 1890 179 1915 Q
6 1 11 3 96 192 1891 263 1910 matrix
6 1 11 4 96 276 1891 315 1910 had
6 1 11 5 96 328 1891 354 1910 20
6 1 11 6 93 367 1891 459 1910 columns
6 1 11 7 93 473 1907 477 1914 ,
6 1 11 8 96 490 1891 540 1910 each
6 1 11 9 96 553 1891 634 1910 column
6 1 11 10 96 647 1891 782 1916 representing
6 1 12 1 96 104 1930 116 1943 a
6 1 12 2 96 127 1928 175 1943 state
6 1 12 3 96 186 1924 225 1943 and
6 1 12 4 96 239 1924 262 1943 10
6 1 12 5 96 273 1930 324 1943 rows
6 1 12 6 96 336 1924 385 1943 each
6 1 12 7 96 396 1924 531 1949 representing
6 1 12 8 96 542 1930 601 1949 every
6 1 12 9 96 613 1924 684 1943 action.
6 1 12 10 96 697 1924 782 1949 Initially
6 1 13 1 10 105 1957 143 1980 the
6 1 13 2 89 158 1957 176 1981 Q
6 1 13 3 92 190 1957 268 1976 -values
6 1 13 4 96 283 1963 334 1976 were
6 1 13 5 96 349 1957 443 1976 assumed
6 1 13 6 96 457 1960 477 1976 to
6 1 13 7 96 490 1957 515 1976 be
6 1 13 8 96 530 1958 542 1977 0
6 1 13 9 96 556 1957 595 1976 and
6 1 13 10 96 608 1957 692 1976 random
6 1 13 11 96 706 1957 783 1976 actions
6 1 14 1 96 105 1996 156 2009 were
6 1 14 2 96 168 1990 265 2015 specified
6 1 14 3 97 276 1990 307 2009 for
6 1 14 4 96 318 1996 376 2015 every
6 1 14 5 96 389 1994 437 2009 state
6 1 14 6 96 449 1990 469 2009 in
6 1 14 7 96 480 1990 512 2009 the
6 1 14 8 96 523 1990 591 2015 policy
6 1 14 9 73 602 1997 617 2009 7
6 1 14 10 89 630 2006 633 2009 .
6 1 14 11 96 645 1990 687 2009 The
6 1 14 12 96 698 1990 782 2015 training
6 1 15 1 96 104 2029 145 2042 was
6 1 15 2 96 154 2023 207 2042 done
6 1 15 3 96 216 2023 248 2042 for
6 1 15 4 96 259 2023 310 2043 1500
6 1 15 5 96 319 2023 413 2048 episodes
6 1 15 6 95 422 2023 461 2042 and
6 1 15 7 95 470 2023 490 2042 in
6 1 15 8 96 499 2023 549 2042 each
6 1 15 9 96 557 2023 647 2048 episode,
6 1 15 10 97 657 2023 689 2042 the
6 1 15 11 96 698 2023 782 2048 training
7 1 1 1 95 352 1593 357 1710 " "
8 1 1 1 96 987 624 1028 649 Fig.
8 1 1 2 96 1040 624 1059 643 4:
8 1 1 3 96 1072 624 1166 643 Rewards
8 1 1 4 97 1177 624 1209 643 for
8 1 1 5 96 1219 624 1313 643 different
8 1 1 6 96 1324 631 1339 643 a
9 1 1 1 96 825 727 865 740 was
9 1 1 2 96 877 721 958 740 iterated
9 1 1 3 96 970 722 1023 741 2000
9 1 1 4 93 1034 721 1092 740 times
9 1 1 5 92 1104 737 1107 740 .
9 1 1 6 96 1120 722 1146 740 At
9 1 1 7 96 1157 721 1189 740 the
9 1 1 8 96 1200 721 1309 746 beginning
9 1 1 9 96 1320 721 1344 740 of
9 1 1 10 96 1352 721 1402 740 each
9 1 1 11 96 1413 721 1502 746 episode,
9 1 2 1 96 824 755 856 774 the
9 1 2 2 96 873 755 988 774 simulation
9 1 2 3 96 1005 761 1045 774 was
9 1 2 4 95 1061 755 1169 774 refreshed.
9 1 2 5 96 1187 755 1298 774 Whenever
9 1 2 6 93 1314 755 1346 774 the
9 1 2 7 92 1361 755 1437 774 robot’s
9 1 2 8 96 1454 758 1503 774 state
9 1 3 1 96 824 788 925 807 exceeded
9 1 3 2 96 938 788 970 807 the
9 1 3 3 96 982 788 1033 807 limit
9 1 3 4 96 1045 788 1060 807 it
9 1 3 5 96 1072 794 1112 807 was
9 1 3 6 96 1125 788 1230 813 penalized
9 1 3 7 96 1242 788 1268 813 by
9 1 3 8 96 1281 788 1384 813 assigning
9 1 3 9 96 1396 788 1471 807 reward
9 1 3 10 96 1483 792 1503 807 to
9 1 4 1 74 826 821 885 841 —100
9 1 4 2 92 901 837 903 840 .
9 1 4 3 93 918 821 960 840 The
9 1 4 4 91 974 820 993 845 Q
9 1 4 5 96 1007 821 1066 840 Table
9 1 4 6 97 1080 821 1097 840 is
9 1 4 7 96 1111 821 1197 846 updated
9 1 4 8 97 1210 824 1229 840 at
9 1 4 9 96 1242 821 1291 840 each
9 1 4 10 96 1306 825 1348 846 step
9 1 4 11 96 1362 821 1469 846 according
9 1 4 12 96 1483 825 1503 840 to
9 1 5 1 96 824 854 918 879 equation
9 1 5 2 90 932 854 947 873 1.
9 1 5 3 96 959 854 1001 873 The
9 1 5 4 96 1012 854 1126 879 Algorithm
9 1 5 5 93 1140 854 1147 873 1
9 1 5 6 96 1161 854 1227 873 shows
9 1 5 7 96 1239 854 1271 873 the
9 1 5 8 96 1283 854 1319 873 full
9 1 5 9 96 1331 854 1442 879 algorithm.
9 2 1 1 96 851 887 893 906 The
9 2 1 2 95 913 887 1028 906 simulation
9 2 1 3 96 1048 893 1088 906 was
9 2 1 4 96 1108 893 1143 906 run
9 2 1 5 96 1163 887 1194 906 for
9 2 1 6 96 1213 887 1267 906 three
9 2 1 7 96 1286 887 1380 906 different
9 2 1 8 95 1399 894 1414 906 a
9 2 1 9 96 1435 887 1502 906 values
9 2 2 1 70 826 919 875 945 (0.7,
9 2 2 2 0 886 921 937 944 0.65,
9 2 2 3 0 944 919 987 945 0.8)
9 2 2 4 92 1001 936 1005 943 ,
9 2 2 5 96 1017 920 1065 939 with
9 2 2 6 59 1076 927 1090 944 7
9 2 2 7 96 1102 920 1159 939 value
9 2 2 8 96 1171 920 1195 939 of
9 2 2 9 96 1206 919 1293 946 (0.999).
9 2 2 10 96 1305 920 1347 939 The
9 2 2 11 96 1358 920 1399 945 Fig.
9 2 2 12 96 1411 920 1424 939 4
9 2 2 13 96 1436 920 1502 939 shows
9 2 3 1 96 824 954 856 973 the
9 2 3 2 96 867 954 961 973 Rewards
9 2 3 3 96 972 960 995 973 vs
9 2 3 4 96 1006 954 1104 979 Episodes
9 2 3 5 96 1115 954 1147 973 for
9 2 3 6 96 1157 954 1213 973 those
9 2 3 7 96 1225 960 1257 973 as.
9 2 3 8 96 1269 955 1285 973 It
9 2 3 9 96 1295 954 1312 973 is
9 2 3 10 96 1323 954 1402 973 evident
9 2 3 11 96 1413 954 1459 977 that,
9 2 3 12 96 1470 954 1502 973 the
9 2 4 1 93 824 987 882 1006 robot
9 2 4 2 92 894 987 985 1006 couldn’t
9 2 4 3 96 998 987 1057 1006 reach
9 2 4 4 96 1071 987 1103 1006 the
9 2 4 5 96 1116 991 1178 1012 target
9 2 4 6 96 1191 987 1276 1006 rewards
9 2 4 7 96 1290 987 1359 1006 within
9 2 4 8 96 1372 987 1404 1006 the
9 2 4 9 96 1418 987 1502 1012 training
9 2 5 1 96 824 1020 894 1045 period
9 2 5 2 96 912 1020 943 1039 for
9 2 5 3 96 960 1020 1017 1039 those
9 2 5 4 96 1035 1020 1124 1045 learning
9 2 5 5 96 1142 1024 1199 1039 rates.
9 2 5 6 96 1218 1021 1252 1039 We
9 2 5 7 96 1271 1026 1304 1039 see
9 2 5 8 96 1323 1020 1369 1043 that,
9 2 5 9 95 1387 1020 1419 1039 for
9 2 5 10 95 1436 1020 1468 1039 the
9 2 5 11 96 1487 1027 1502 1039 a
9 2 6 1 96 825 1053 892 1072 values
9 2 6 2 96 909 1053 942 1073 0.7
9 2 6 3 95 958 1053 997 1072 and
9 2 6 4 95 1013 1054 1052 1076 0.8,
9 2 6 5 96 1069 1053 1101 1072 the
9 2 6 6 96 1117 1053 1174 1072 robot
9 2 6 7 96 1189 1053 1271 1072 reaches
9 2 6 8 95 1287 1053 1398 1072 maximum
9 2 6 9 96 1413 1053 1503 1078 possible
9 2 7 1 95 824 1086 963 1105 accumulated
9 2 7 2 96 975 1086 1067 1109 rewards,
9 2 7 3 96 1080 1086 1126 1109 200,
9 2 7 4 96 1140 1086 1209 1105 within
9 2 7 5 96 1222 1086 1262 1105 400
9 2 7 6 96 1275 1086 1375 1111 episodes.
9 2 7 7 96 1388 1086 1430 1105 The
9 2 7 8 96 1443 1092 1503 1105 curve
9 2 8 1 96 824 1120 872 1139 with
9 2 8 2 77 883 1127 899 1139 a
9 2 8 3 95 911 1120 968 1139 value
9 2 8 4 96 980 1120 1014 1140 0.7
9 2 8 5 96 1025 1120 1041 1139 is
9 2 8 6 96 1053 1120 1092 1139 less
9 2 8 7 96 1105 1120 1167 1139 stable
9 2 8 8 96 1178 1120 1286 1145 compared
9 2 8 9 96 1297 1123 1317 1139 to
9 2 8 10 96 1328 1120 1369 1139 that
9 2 8 11 96 1379 1120 1403 1139 of
9 2 8 12 95 1412 1121 1452 1140 0.8.
9 2 8 13 96 1464 1121 1503 1139 But
9 2 9 1 96 824 1153 866 1172 The
9 2 9 2 96 879 1159 939 1172 curve
9 2 9 3 96 953 1153 1001 1172 with
9 2 9 4 77 1015 1160 1030 1172 a
9 2 9 5 96 1046 1153 1102 1172 value
9 2 9 6 96 1116 1153 1163 1173 0.65
9 2 9 7 96 1178 1159 1237 1172 never
9 2 9 8 96 1250 1153 1332 1172 reaches
9 2 9 9 96 1346 1153 1378 1172 the
9 2 9 10 96 1392 1153 1503 1172 maximum
9 2 10 1 96 824 1186 963 1205 accumulated
9 2 10 2 96 973 1186 1054 1205 reward.
10 1 1 1 90 824 1238 844 1256 B.
10 1 1 2 92 861 1238 919 1262 Deep
10 1 1 3 92 931 1238 949 1261 Q
10 1 1 4 93 958 1237 1053 1256 Network
10 1 1 5 91 1063 1238 1138 1261 (DQN)
11 1 1 1 90 852 1280 869 1298 V
11 1 1 2 90 884 1279 943 1298 Mnih
11 1 1 3 96 956 1282 976 1298 et
11 1 1 4 96 989 1279 1007 1298 al
11 1 1 5 96 1023 1279 1051 1302 [9]
11 1 1 6 96 1066 1279 1108 1298 first
11 1 1 7 96 1122 1279 1171 1298 used
11 1 1 8 96 1185 1280 1242 1304 Deep
11 1 1 9 96 1255 1279 1354 1304 Learning
11 1 1 10 95 1368 1285 1389 1298 as
11 1 1 11 95 1398 1275 1409 1308 a
11 1 1 12 95 1428 1279 1503 1298 variant
11 1 2 1 91 824 1312 848 1331 of
11 1 2 2 91 858 1312 876 1336 Q
11 1 2 3 96 888 1312 986 1337 Learning
11 1 2 4 95 999 1312 1105 1337 algorithm
11 1 2 5 95 1117 1315 1137 1331 to
11 1 2 6 96 1148 1312 1194 1337 play
11 1 2 7 96 1207 1312 1238 1331 six
11 1 2 8 96 1249 1318 1318 1337 games
11 1 2 9 96 1330 1312 1354 1331 of
11 1 2 10 96 1364 1312 1419 1331 Atari
11 1 2 11 93 1432 1312 1485 1331 2600
11 1 2 12 93 1498 1328 1502 1335 ,
11 1 3 1 96 825 1345 890 1364 which
11 1 3 2 96 901 1345 1051 1370 outperformed
11 1 3 3 96 1062 1345 1088 1364 all
11 1 3 4 97 1099 1345 1155 1364 other
11 1 3 5 96 1165 1345 1258 1370 previous
11 1 3 6 92 1270 1345 1392 1370 algorithms.
11 1 3 7 92 1406 1361 1408 1364 .
11 1 3 8 96 1421 1346 1442 1364 In
11 1 3 9 96 1454 1345 1503 1364 their
11 1 4 1 96 824 1384 889 1403 paper,
11 1 4 2 95 901 1381 941 1397 two
11 1 4 3 96 952 1378 1026 1403 unique
11 1 4 4 96 1037 1378 1160 1403 approaches
11 1 4 5 96 1172 1384 1223 1397 were
11 1 4 6 96 1235 1378 1290 1397 used.
11 2 1 1 80 852 1421 861 1429 «
11 2 1 2 96 880 1411 1003 1436 Experience
11 2 1 3 96 1014 1411 1090 1436 Replay
11 3 1 1 19 852 1455 861 1463 e
11 3 1 2 96 880 1445 995 1464 Derivation
11 3 1 3 93 1006 1445 1030 1464 of
11 3 1 4 93 1039 1445 1058 1469 Q
11 3 1 5 96 1069 1445 1141 1464 Values
11 3 1 6 96 1152 1445 1172 1464 in
11 3 1 7 96 1183 1451 1222 1464 one
11 3 1 8 96 1233 1445 1319 1464 forward
11 3 1 9 96 1330 1451 1376 1470 pass
12 1 1 1 96 851 1516 893 1535 The
12 1 1 2 96 908 1516 1014 1541 technique
12 1 1 3 96 1028 1516 1052 1535 of
12 1 1 4 96 1064 1516 1187 1541 Experience
12 1 1 5 96 1202 1516 1283 1541 Replay,
12 1 1 6 96 1299 1516 1427 1541 experiences
12 1 1 7 96 1442 1516 1465 1535 of
12 1 1 8 96 1478 1522 1503 1535 an
12 1 2 1 92 824 1552 883 1574 agent
12 1 2 2 92 892 1565 896 1572 ,
12 1 2 3 89 905 1549 936 1568 i.e.
12 1 2 4 95 948 1548 1022 1575 (state,
12 1 2 5 94 1030 1549 1124 1573 reward,
12 1 2 6 93 1132 1550 1212 1573 action,
12 1 2 7 62 1220 1548 1330 1575 statenew)
12 1 2 8 96 1340 1555 1372 1568 are
12 1 2 9 96 1382 1549 1447 1568 stored
12 1 2 10 96 1456 1555 1503 1568 over
12 1 3 1 96 824 1588 883 1607 many
12 1 3 2 93 897 1582 990 1607 episodes
12 1 3 3 92 1005 1598 1008 1601 .
12 1 3 4 96 1023 1583 1044 1601 In
12 1 3 5 96 1058 1582 1090 1601 the
12 1 3 6 96 1103 1582 1192 1607 learning
12 1 3 7 96 1205 1582 1281 1607 period,
12 1 3 8 95 1295 1582 1345 1601 after
12 1 3 9 95 1357 1582 1407 1601 each
12 1 3 10 96 1420 1582 1503 1607 episode
12 1 4 1 96 824 1615 908 1634 random
12 1 4 2 96 918 1615 1000 1634 batches
12 1 4 3 96 1012 1615 1035 1634 of
12 1 4 4 96 1045 1615 1090 1634 data
12 1 4 5 96 1101 1615 1154 1634 from
12 1 4 6 96 1165 1615 1283 1640 experience
12 1 4 7 96 1294 1621 1326 1634 are
12 1 4 8 96 1339 1615 1387 1634 used
12 1 4 9 96 1399 1619 1419 1634 to
12 1 4 10 95 1431 1615 1503 1640 update
12 1 5 1 96 824 1648 856 1667 the
12 1 5 2 96 868 1648 941 1667 model.
12 1 5 3 96 955 1648 990 1671 [9].
12 1 5 4 96 1002 1648 1065 1667 There
12 1 5 5 96 1077 1654 1109 1667 are
12 1 5 6 96 1121 1648 1195 1667 several
12 1 5 7 96 1207 1648 1291 1667 benefits
12 1 5 8 96 1303 1648 1326 1667 of
12 1 5 9 97 1336 1648 1385 1667 such
12 1 5 10 96 1396 1648 1501 1673 approach.
12 1 6 1 96 824 1682 940 1707 According
12 1 6 2 96 951 1686 971 1701 to
12 1 6 3 96 982 1682 1014 1701 the
12 1 6 4 96 1025 1688 1090 1707 paper,
12 2 1 1 55 852 1730 861 1738 «
12 2 1 2 96 880 1721 896 1739 It
12 2 1 3 95 906 1720 976 1739 allows
12 2 1 4 96 987 1723 1063 1745 greater
12 2 1 5 96 1073 1720 1119 1739 data
12 2 1 6 96 1129 1720 1235 1745 efficiency
12 2 1 7 96 1247 1726 1268 1739 as
12 2 1 8 96 1279 1720 1329 1739 each
12 2 1 9 96 1340 1723 1383 1745 step
12 2 1 10 93 1394 1720 1417 1739 of
12 2 1 11 93 1426 1720 1502 1745 experi-
12 3 1 1 96 880 1759 929 1772 ence
12 3 1 2 96 940 1759 977 1772 can
12 3 1 3 96 988 1753 1013 1772 be
12 3 1 4 96 1024 1753 1074 1772 used
12 3 1 5 96 1085 1753 1105 1772 in
12 3 1 6 96 1116 1759 1175 1778 many
12 3 1 7 96 1187 1753 1261 1778 weight
12 3 1 8 96 1271 1753 1354 1778 updates
12 4 1 1 62 852 1796 861 1804 «
12 4 1 2 92 880 1786 1027 1811 Randomizing
12 4 1 3 96 1037 1786 1119 1805 batches
12 4 1 4 96 1129 1786 1200 1805 breaks
12 4 1 5 96 1210 1786 1339 1805 correlations
12 4 1 6 92 1349 1786 1440 1805 between
12 4 1 7 92 1451 1792 1502 1805 sam-
12 5 1 1 96 879 1820 922 1845 ples
12 6 1 1 34 852 1863 861 1871 «
12 6 1 2 96 880 1853 994 1872 Behaviour
12 6 1 3 96 1010 1853 1135 1872 distribution
12 6 1 4 96 1152 1853 1169 1872 is
12 6 1 5 96 1186 1853 1283 1878 averaged
12 6 1 6 96 1300 1859 1347 1872 over
12 6 1 7 96 1364 1859 1423 1878 many
12 6 1 8 94 1440 1853 1463 1872 of
12 6 1 9 94 1478 1853 1503 1872 its
12 7 1 1 96 879 1886 973 1911 previous
12 7 1 2 95 985 1890 1044 1905 states
12 7 2 1 96 824 1925 846 1943 In
12 7 2 2 96 859 1924 891 1943 the
12 7 2 3 89 904 1924 996 1943 classical
12 7 2 4 89 1009 1924 1028 1948 Q
12 7 2 5 96 1041 1924 1130 1949 learning
12 7 2 6 96 1143 1924 1249 1949 approach,
12 7 2 7 96 1263 1930 1301 1943 one
12 7 2 8 96 1314 1924 1349 1943 has
12 7 2 9 96 1363 1927 1382 1943 to
12 7 2 10 96 1395 1924 1440 1949 give
12 7 2 11 96 1454 1927 1503 1943 state
12 7 3 1 96 824 1957 863 1976 and
12 7 3 2 96 877 1957 943 1976 action
12 7 3 3 96 958 1963 979 1976 as
12 7 3 4 96 994 1963 1018 1976 an
12 7 3 5 96 1033 1957 1089 1982 input
12 7 3 6 96 1102 1957 1198 1982 resulting
12 7 3 7 93 1212 1957 1232 1976 in
12 7 3 8 82 1247 1956 1267 1981 Q
12 7 3 9 96 1282 1957 1339 1976 value
12 7 3 10 96 1354 1957 1385 1976 for
12 7 3 11 96 1399 1957 1440 1976 that
12 7 3 12 96 1454 1960 1502 1976 state
12 7 4 1 96 824 1990 863 2009 and
12 7 4 2 96 875 1990 946 2009 action.
12 7 4 3 96 960 1990 1086 2015 Replicating
12 7 4 4 96 1098 1990 1136 2009 this
12 7 4 5 96 1149 1990 1249 2015 approach
12 7 4 6 96 1261 1990 1281 2009 in
12 7 4 7 96 1293 1990 1366 2009 Neural
12 7 4 8 96 1378 1990 1474 2009 Network
12 7 4 9 96 1486 1990 1502 2009 is
12 7 5 1 96 824 2023 960 2048 problematic.
12 7 5 2 96 974 2024 1064 2042 Because
12 7 5 3 96 1076 2023 1096 2042 in
12 7 5 4 96 1108 2023 1148 2042 that
12 7 5 5 96 1159 2029 1205 2042 case
12 7 5 6 95 1217 2029 1256 2042 one
12 7 5 7 95 1267 2023 1303 2042 has
12 7 5 8 96 1315 2027 1335 2042 to
12 7 5 9 96 1347 2023 1391 2048 give
12 7 5 10 97 1404 2026 1452 2042 state
12 7 5 11 96 1465 2023 1503 2042 and
