4 Algorithm analysis 4.1 Memory requirement In this section, we elaborate on the memory consumption of the proposed method in comparison with SEAGLE. First, let us state that gradient based methods, such as NAGD or CG, have similar memory requirements. It corresponds roughly to three times the size of the optimization variable which is the part that is common to both algorithms. The additional memory requirement that is specific to SEAGLE relies only on the storage of the NAGD iterates during the forward computation. Suppose that Knaap € N iterations are necessary to compute the forward model with and that the region 2 is sampled over N € N pixels . Since the total field u, computed by NAGD is complex-valued, each pixel is represented with 16 bytes . Hence, the difference of memory consumption between SEAGLE and our method is which corresponds to the storage of the Knacp intermediate iterates of NAGD. Here, we assumed that WD was computed by sequentially adding the partial gradients VD, associated to the P incident fields. Hence, once the partial gradient associated to one incident angle is computed by successively applying the forward model and the error-backpropagation procedure, the memory used to store the intermediate iterates can be recycled to compute the partial gradient associated to the next incident angle. However, when the parallelization strategy detailled in Section is used, the memory requirement is mutiplied by the number N-nreaas € N of threads, so that Indeed, since the threads of a single computer share memory, computing Nrnreads partial gradients in parallel requires Nrnreads times more memory. For illustration, we give in Fig. 2 the evolution of Amem as a function of N for different values of Kyacp and Nppreads: One can see with the vertical dashed lines that, for 3D volumes, the memory used by SEAGLE quickly reaches several tens of Megabytes, even for small volumes , to hundreds of Gigabytes for the larger volumes that are typical of microscopy . This shows the limitation of SEAGLE for 3D reconstruction in the presence of a shortage of memory resources and reinforces the interest of the proposed alternative. 4.2 Conjugate gradient vs. Nesterov accelerated gradient descent for Due to Proposition 3.1, we can compute both and J/# using any state-ofthe-art quadratic optimization algorithm. This contrasts with SEAGLE, where one must derive the error-backpropagation rule from the forward algorithm, which may limit its choice. We now provide numerical evidence that GC is more efficient than NAGD for solving . To this end, we consider a circular object of radius rpeaa 9