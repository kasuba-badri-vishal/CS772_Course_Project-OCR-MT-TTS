KBSF computational cost becomes a function of m only. In particular, the cost of solving M through dynamic programming becomes polynomial in m instead of n: while one application of 7, the Bellman operator of M, is O, the computation of T is O. Therefore, KBSF’s time and memory complexities are only linear in n. We note that, in practice, KBSF’s computational requirements can be reduced even further if one enforces the kernels «2 and &; to be sparse. In particular, given a fixed 3;, instead of computing k; for j = 1,2,...,Nq@, one can evaluate the kernel on a pre-specified neighborhood of 3; only. Assuming that k- is zero for all sf outside this region, one avoids not only computing the kernel but also storing the resulting values for a fixed 8¢. 4.1 <A closer look at KBSF’s approximation As outlined in Section , KBRL defines the probability of a transition from state 3b to state 8@ as being «2, where a,b € A . Note that the kernel k® is computed with the initial state sf, and not 8? itself. The intuition behind this is simple: since we know the transition sf 4 sf, has occurred before, the more “similar” g is to sf, the more likely the transition 8? 4 8* becomes . From , it is clear that the computation of matrices K® performed by KBSF follows the same reasoning underlying the computation of KBRL’s matrices P*; in particular, K2 gives the probability of a transition from 5; to 8¢. However, when we look at matrix D things are slightly different: here, the probability of a “transition” from 3b to representative state 5; is given by Kr—a computation that involves 3; itself. If we were to strictly adhere to KBRL’s logic when computing the transition probabilities to the representative states 8;, the probability of transitioning from Be to 8; upon executing action a should be a function of Ey and a state s’ from which we knew a transition s’ 8; had occurred. In this case we would end up with one matrix D® for each action a € A. Note though that this formulation of the method is not practical, because the computation of the matrices D* would require a transition 4 3; for each a € A and each 5; € S. Clearly, such a requirement is hard to fulfill even if we have a generative model available to generate sample transitions. In this section we provide an interpretation of the approximation computed by KBSF that supports our definition of matrix D. We start by looking at how KBRL constructs the matrices P*. As shown in F igure 2a, for each action a € A the state 3b has an associated stochastic vector pf € R!*" whose nonzero entries correspond to the kernel % 1? evaluated at sf,k = 1,2,...,nq. Since we are dealing with a continuous state space, it is possible to compute an analogous vector for any s € S and any a € A. Focusing on the nonzero entries of BF, we define the function Clearly, full knowledge of the function Pa allows for an exact computation of KBRL’s transition matrix P®. Now suppose we do not know Pga and we want to compute an 13