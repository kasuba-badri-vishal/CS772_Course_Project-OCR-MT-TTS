 et et Table 1: Language modeling for , = 1725. Dense and sparse variants have the same number of parameters for N = 3 and 7 = 0.555. These values are obtained by identifying both expressions. Note that the equality in model parameters for the dense and sparse case holds only approximately due to rounding errors in and . Figure 1 displays Wy; and W», for the middle layer, which has close to 11M parameters out of the total of 24M in the whole model. A dense model with hidden size h = 1725 would require 46M parameters, with 24M in the middle LSTM alone. Given the strong hyperparameter dependence of the AWD-LSTM model, and the known issues in objectively evaluating language models , we decided to keep all hyperparameters as in the implementation from Merity et al. , including the weight dropping with p = 0.5 in the sparse Wp,;, matrices. Table 1 shows the test perplexity on a processed version of the Penn Treebank , both with and without the ‘finetune’ step®, displaying mean and standard deviation over 5 different runs. Without finetuning, the sparse model consistently performs around | perplexity point better, whereas after finetuning, the original remains slightly better, although less consistently so over different random seeds. We observed that the sparse model overfits more strongly than the baseline, especially during the finetune step. We hypothesize that the regularization effect of a priori limiting interac5Our implementation extends https: github. com/salesforce/awd-1stm-1m ®The ‘finetune’ step indicates hot-starting the Averaged Stochastic Gradient Descent optimization once more, after convergence in the initial optimization step . tions between dimensions does not compensate for the increased expressiveness of the model due to the larger hidden state size. Further experimentation, with tuned hyperparameters, is needed to determine the actual benefits of predefined sparseness, in terms of model size, resulting perplexity, and sensitivity to the choice of hyperparameters. 4 Sparse Word Embeddings Given a vocabulary with V words, we want to construct vector representations of length k for each word such that the total number of parameters needed , is smaller than kV. We introduce one way to do this based on word frequencies , and present part-ofspeech tagging experiments . 4.1 Word-Frequency based Embedding Size Predefined sparseness in word embeddings amounts to deciding which positions in the word embedding matrix E ¢ RY ** should be fixed to zero, prior to training. We define the fraction of trainable entries in E as the embedding density dz. We hypothesize that rare words can be represented with fewer parameters than frequent words, since they only appear in very specific contexts. This will be investigated experimentally in Section . Word occurrence frequencies have a typical Zipfian nature , with many rare and few highly frequent terms. Thus, representing the long tail of rare terms with short embeddings should greatly reduce memory requirements. In the case of a low desired embedding density dz, we want to save on the rare words, in terms of assigning trainable parameters, and focus on the fewer more popular words. An exponential decay in the number of words that are assigned longer representations is one possible way to implement this. In other words, we propose to have the number of words that receive a trainable parameter at dimension j decrease with a factor a 0, 1. For a given fraction 6, the parameter a can be determined from requiring the total number of nonzero embedding parameters to amount to a given fraction 6 of all parameters: LZ and numerically solving for a. Figure 2 gives examples of embedding matrices with varying dg. For a vocabulary of 44k terms