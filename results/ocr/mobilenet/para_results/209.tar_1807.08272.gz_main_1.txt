Fig. 3: Controller Block Diagram IV. REINFORCEMENT LEARNING METHODS AS In , we worked on traditional Controllers like PID, Fuzzy PD,PD+I & LQR The biggest problem with those methods is that they need to be tuned manually. So, reaching CONTROLLERS Fig. 4: Rewards for different a optimal values of Controllers depends on many trials and was iterated 2000 times At the beginning of each episode, errors. Many a times optimum values aren't reached at all. the simulation was refreshed. Whenever the robot's state The biggest benefit of Reinforcement learning algorithms exceeded the limit it was penalized by assigning reward to as Controllers is that the model tunes itself to reach the -100 The Q Table is updated at each step according to Optimum values. The following two sections discuss Q equation 1. The Algorithm 1 shows the full algorithm. Qlearning was developed by Christopher John Cornish the Rewards VS Episodes for those as. Itis evident that, the Hellaby Watkins . According to Watkins, "it provides robot couldn't reach the target rewards within the training agents with the capability of learning to act optimally in period for those learning rates. We see that, for the a Markovian domains by experiencing the consequences of values 0.7 and 0.8, the robot reaches maximum possible actions, without requiring them to build maps of the doaccumulated rewards, 200, within 400 episodes. The curve mains." . In a Markovian domain, Q functionthe model with a value 0.7i is less stable compared to that of 0.8. But to be generated using the algorithmcalculates the expected The curve with a value 0.65 never reaches the maximum Learning and Deep Q Network. A. Q Learning The simulation was run for three different a values with 7 value of . The Fig. 4 shows utility for a given finite state S and every possible finite accumulated reward. the optimum action a having the highest value of Q B. Deep Q Network action a. The agent which is the robot in this caseselects this action choosing rule is also called Policy. Initially, V Mnih et al first used Deep Learning as a variant the Q function values are assumed to be zero. After ofQ Learning algorithm to play six games of Atari 2600 every training step , the values are updated according to the which outperformed all other previous algorithms. In their following equation paper, two unique approaches were used. Experience Replay Derivation of Q Values in one forward pass The technique of Experience Replay, experiences of an The objective for the Model in our project is to keep agent. ,i.e. are stored over it within limits i.e. +50 At first the robot model, Q many episodes In the learning period, after each episode matrix, Policy T are initialized There are some interrandom batches of data from experience are used to update esting points to make. The states are not finite. Within the model. . There are several benefits of such approach. limit range * hundreds and thousands of pitch angles are According to the paper, possible. Having thousands of columns is not possible. So, the state values were discretized. We discretized the values to 20 discrete state angles from -10° to 10°. For action value, we chose 10 different velocities. They are ms The Q matrix had 20 columns * each column representing It allows greater data efficiency as each step of experience can be used in many weight updates Randomizing batches breaks correlations between samBehaviour distribution is averaged over many of its ples previous states a state and 10 rows each representing every action. Initially In the classical Q learning approach, one has to give state the Q -values were assumed to be 0 and random actions and action as an input resulting in Q value for that state were specified for every state in the policy T The training and action. Replicating this approach in Neural Network is was done for 1500 episodes and in each episode, the training problematic. Because in that case one has to give state and