4 Algorithm analysis 4.1 Memory requirement In this section, we elaborate on the memory consumption of the proposed method in comparison with SEAGLE. First, let us state that gradient based methods, such as NAGD or CG, have similar memory requirements. It corresponds roughly to three times the size of the optimization variable which is the part that is common to both algorithms. The additional memory requirement that is specific to SEAGLE relies only on the storage of the NAGD iterates during the forward computation. Suppose that ANAGD € N iterations are necessary to compute the forward model with and that the region 2 is sampled over N € N pixels . Since the total field u, computed by NAGD is complex-valued, each pixel is represented with 16 bytes . Hence, the difference of memory consumption between SEAGLE and our method is which corresponds to the storage of the KNAGD intermediate iterates of NAGD. Here, we assumed that VD was computed by sequentially adding the partial gradients VD, associated to the P incident fields. Hence, once the partial gradient associated to one incident angle is computed by successively applying the forward model and the eror-backpropacation procedure, the memory used to store the intermediate iterates can be recycled to compute the partial gradient associated to the next incident angle. However, when the parallelization strategy detailled in Section is used, the memory requirement is mutiplied by the number NThreads E N of threads, SO that Indeed, since the threads of a single computer share memory, computing NThreads For illustration, we give in Fig. 2 the evolution of AMem as a function of N for different values of KNAGD and NThreadsOne can see with the vertical dashed lines that, for 3D volumes, the memory used by SEAGLE quickly reaches several tens of Megabytes, even for small volumes , to hundreds of Gigabytes for the larger volumes that are typical of microscopy . This shows the limitation of SEAGLE for 3D reconstruction in the presence of a shortage of memory resources and reinforces the interest of the proposed alternative. 4.2 Conjugate gradient US. Nesterov accelerated gradient deDue to Proposition 3.1, we can compute both and 3E using any state-ofthe-art quadratic optimization algorithm. This contrasts with SEAGLE, where one must derive the error-backpropacation rule from the forward algorithm, which may limit its choice. We now provide numerical evidence that GC is more efficient than NAGD for solving . To this end, we consider a circular object of radius Tbead partial gradients in parallel requires NThreads times more memory. scent for 9